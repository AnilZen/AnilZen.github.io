<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Anıl Zenginoğlu</title><link>https://anilzen.github.io/</link><atom:link href="https://anilzen.github.io/index.xml" rel="self" type="application/rss+xml"/><description>Anıl Zenginoğlu</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 08 Dec 2022 00:01:00 +0000</lastBuildDate><image><url>https://anilzen.github.io/media/sharing.jpg</url><title>Anıl Zenginoğlu</title><link>https://anilzen.github.io/</link></image><item><title>Learning Machine Learning</title><link>https://anilzen.github.io/post/2022/learning-machine-learning/</link><pubDate>Thu, 08 Dec 2022 00:01:00 +0000</pubDate><guid>https://anilzen.github.io/post/2022/learning-machine-learning/</guid><description>&lt;a target="_blank" href="https://colab.research.google.com/drive/1A80Z_o55cJI-PkHjsg-j2sUiwsx3A9EY?usp=sharing">
&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
&lt;/a>
&lt;h2 id="neural-network-what-is-machine-learning">Neural Network: What is machine learning?&lt;/h2>
&lt;p>The core of a machine learning algorithm is the neural network that maps inputs to desired outputs. The term &amp;ldquo;neural network&amp;rdquo; calls to mind structures in the human brain with layers of interconnected neurons working together to solve problems. But in machine learning, we are simply talking about a function with parameters. &amp;ldquo;Learning&amp;rdquo; is adjusting these parameters until the difference between the desired output and the actual output of the function is sufficiently small. That&amp;rsquo;s it.&lt;/p>
&lt;p>This was a very short summary of what people mean by machine learning. You can imagine that the function, the parameters, and the adjustment process are all rather sophisticated and can get very complicated. I&amp;rsquo;ll expand on this basic idea below with an example of a machine-learning algorithm.&lt;/p>
&lt;p>First, some terminology. Think of each input dimension as a neuron in a neural network. The parameters of the neural network, or the function, are called weights and biases. Weights represent the strength of the connection between neurons; biases shift the activation threshold of a neuron. By adjusting the weights and biases based on the output, the network learns the patterns in the data and makes predictions on new data.&lt;/p>
&lt;p>Let&amp;rsquo;s write this down. I mentioned that the neural network is just a function with parameters. Its output is usually a probability, so we&amp;rsquo;ll call it $p$. The network should look something like $p=f(x; W,b)$, where $x$ is the input array, $p$ is the network&amp;rsquo;s output array, $W$ are the weights, and $b$ are the biases. A simple neural network could then be written like this
$$ p = W \cdot x + b. $$
But wait, you say; this is just a linear transformation! Layering linear transformations on top of each other can only create a linear network. You can&amp;rsquo;t learn complex patterns and make accurate predictions with just linear transformations. To introduce nonlinearity into the model, we throw this into a nonlinear activation function $\sigma$, so the output looks like
$$ p = \sigma(W \cdot x + b). \label{1} \tag{1} $$
There are a few commonly used activation functions that one frequently encounters: the sigmoid function, the hyperbolic tangent (tanh) function, the rectified linear unit (ReLU) function, and so on. We&amp;rsquo;ll use a generalization of the sigmoid (or logistic) function for our experiments.
$$ \sigma(x) = \frac{1}{1 + e^{-x}} $$
This function maps a real-valued input to a value between 0 and 1, so the output can be interpreted as a probability, making it directly useful in classification problems.&lt;/p>
&lt;p>To get into more detail, we need to understand and prepare the input. I use the MNIST dataset for the demonstration below. We will avoid the powerful machine learning packages and only use NumPy&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, so all operations can be considered elemental. You can follow along on &lt;a href="https://colab.research.google.com/drive/1A80Z_o55cJI-PkHjsg-j2sUiwsx3A9EY?usp=sharing" target="_blank" rel="noopener">Colab&lt;/a>.
&lt;a target="_blank" href="https://colab.research.google.com/drive/1A80Z_o55cJI-PkHjsg-j2sUiwsx3A9EY?usp=sharing">
&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
&lt;/a>&lt;/p>
&lt;h2 id="mnist-dataset-prepare-the-input">MNIST Dataset: Prepare the input&lt;/h2>
&lt;p>The MNIST dataset is a large database of handwritten digits consisting of 60,000 training images and 10,000 test images. Each image is a 28x28 grayscale image labeled with the correct digit, from 0 to 9. It&amp;rsquo;s commonly used for training and testing various image processing and machine learning algorithms. The digits in grayscale look like this
&lt;figure id="figure-handwritten-digits-0-and-1-from-the-mnist-dataset">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Handwritten digits from the MNIST dataset" srcset="
/post/2022/learning-machine-learning/digits_hu2b0dc1340f2bd946add58c7b992aeb19_6240_1188d41ad3fd793b572412ff33b047b3.webp 400w,
/post/2022/learning-machine-learning/digits_hu2b0dc1340f2bd946add58c7b992aeb19_6240_3fc866dcf8d288cdcdc65060faa475c2.webp 760w,
/post/2022/learning-machine-learning/digits_hu2b0dc1340f2bd946add58c7b992aeb19_6240_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/learning-machine-learning/digits_hu2b0dc1340f2bd946add58c7b992aeb19_6240_1188d41ad3fd793b572412ff33b047b3.webp"
width="600"
height="400"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Handwritten digits 0 and 1 from the MNIST dataset.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Our goal is to teach the single-layer network (\ref{1}) to recognize these handwritten digits. The network learns from the training dataset by adjusting its weights to minimize the difference between the desired output and the actual output, that is, the loss. This process is repeated until the loss is sufficiently small, implying that the network has learned the dataset. So we need to define the loss, calculate how it depends on layer parameters, and find a way to minimize it iteratively.&lt;/p>
&lt;h2 id="layer-and-loss-build-the-model">Layer and Loss: Build the model&lt;/h2>
&lt;p>Our model architecture consists of just the one layer in (\ref{1}). So this is an example of shallow learning. But even with shallow networks, it can get confusing with the number of samples, inputs, and outputs. To recap, we have $M=60,000$ samples in the training set; each sample has $N=28\times28=784$ dimensions (one for each pixel in a flattened 1D-array); the output has $K=$10 dimensions (one for each digit). Accounting is worse when you have hidden layers in between. They all live in different spaces, so it makes sense to introduce different types of letters into the tensor notation for each type of space. To clarify each space, I like to define indices with their own ranges&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>:
$$ \begin{align}
a,b&amp;amp;=1,2,\dots,M=60,000. \\
\alpha, \beta&amp;amp;=1,2,\dots,N=784. \\
i,j&amp;amp;=1,2,\dots,K=10.
\end{align} $$
We can then write the output of our AI algorithm as
$$ p_{ai} = \sigma(z_{ai}) = \sigma\left( \sum_{\alpha=1}^{N} x_{a\alpha} W_{\alpha i} + b_i \right). \tag{2} \label{2} $$
Here, $\sigma$ is a generalization of the sigmoid function, called the softmax function. In code, we write&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">p_ai&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">z&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">W&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">b&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This step is sometimes called the forward pass.&lt;/p>
&lt;p>We do not use the sigmoid function because we have to ensure that the output probabilities sum to one. The softmax function is defined as
$$\sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}. \tag{3} \label{3} $$
It maps a vector of arbitrary real values, $z_i$, to a vector of values between 0 and 1. The sum of all outputs is 1, so each output can be interpreted as a probability. This makes it a useful activation function for multiclass classification tasks, where the predicted probabilities must sum to 1.&lt;/p>
&lt;p>The softmax function is numerically unstable if implemented naively, so we rewrite it such that the maximum value of the input array is 0.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">z&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">z&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">axis&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)[:,&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">newaxis&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">axis&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)[:,&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">newaxis&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr />
&lt;p>We now need to devise a way to tell our network when its outputs, $p_{ai}$, are losers. This is done by defining a loss function that measures the difference between the desired output, $y$, and the actual output, $p$. There are many possible choices. For example, when predicting a continuous variable, one typically uses a regression loss function such as mean squared error or mean absolute error. We have a multiclass classification problem (one class for each digit), so we&amp;rsquo;ll use the cross-entropy loss&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> defined as
$$ L = - \frac{1}{M} \sum_{a=1}^M \sum_{i=1}^N y_{ai} \log p_{ai}, \tag{4} \label{4} $$
where $p$ is the predicted probability, $y$ is the actual probability, $M$ is the number of samples, and $N$ is the number of classes. In NumPy&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">cross_entropy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The cross-entropy loss is often used with a softmax activation function in the output layer of a classification model. One of the main reasons is the beautiful simplification of its derivative with respect to the input. But I&amp;rsquo;m getting ahead of myself here.&lt;/p>
&lt;p>How do we minimize the loss? We can evaluate the function at different locations in the parameter space to search for the minimum, but that&amp;rsquo;s not an efficient approach, especially in high dimensions. In our simple toy example, the parameter space of weights and biases has $784\times 10+10=7,850$ dimensions.&lt;/p>
&lt;p>A better approach is gradient descent. We start at some random point which will invariably have a large loss. It&amp;rsquo;s like starting on a big hill. To go to the bottom, you take small steps downhill, that is, you descend along the negative gradient, until you can&amp;rsquo;t go reasonably further. We need to compute the gradient of the loss function with respect to the weights and biases to determine the downhill direction. Using the chain rule, we obtain the following formula for the derivative of the loss function with respect to bias
$$ \frac{\partial{L}}{\partial b_{j}} = - \frac{1}{M}\sum_{a=1}^M \sum_{i=1}^N \sum_{k=1}^N y_{ai}\frac{\partial \log p_{ai}}{\partial z_{ak}} \frac{\partial z_{ak}}{\partial b_j} . $$
This calculation is where the simplification comes in when you combine the cross-entropy loss with softmax activation. To demonstrate, write the total loss as the mean of the losses of all samples, $L=\tfrac{1}{M}\sum_{a=1}^M \ell_a$. Let&amp;rsquo;s compute the derivative for a single sample, suppressing its index
$$ \frac{\partial{\ell_a}}{\partial b_{j}} = - \sum_{i=1}^N \sum_{k=1}^N y_{i}\frac{\partial \log p_{i}}{\partial z_{k}} \frac{\partial z_{k}}{\partial b_j} . $$
The log-term&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> with the definition of softmax (\ref{3}) reads
$$ \log p_i = \log (\sigma(z_i)) = z_i - \log\left(\sum_{j=1}^K e^{z_j}\right).$$
We get for the $z$-derivative
$$ \frac{\partial \log p_i}{\partial z_k} = \delta_{ik} - \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}} = \delta_{ik} - p_k.$$
Combining with the summation over $y$, we obtain this very simple formula
$$ \sum_{i=1}^{N} y_{i} (\delta_{ik} - p_k) = y_k - p_k \sum_{i=1}^{N} y_{i} = y_k - p_k. $$
For the last step, remember that $y_i$ are probabilities that sum up to 1. We then have
$$ \frac{\partial{\ell_a}}{\partial b_{j}} = \sum_{k=1}^N (p_k - y_k)\frac{\partial z_{k}}{\partial b_j} $$
Now we can insert the dependence of $z$ on $b$ and bring back the summation over the samples with index $a$ to get
$$ \frac{\partial{L}}{\partial b_{j}} = \frac{1}{M} \sum_{a=1}^M (p_{aj}-y_{aj}). $$
Similarly, for the weights
$$ \frac{\partial{L}}{\partial W_{\beta j}} = \frac{1}{M} \sum_{a=1}^M (p_{aj}-y_{aj}) x_{a\beta}. $$
In NumPy, the gradient of the loss is then calculated by&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">gradient&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dL&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">db&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dL&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">axis&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dW&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dL&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">dW&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">db&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We then update the weights and biases based on these gradients with some step size $\lambda$. What mathematicians call step size is called learning rate in machine learning, even though it&amp;rsquo;s not quite a rate. Anyway, we update the weights and biases iteratively as follows
$$ \begin{align}
W^{n+1} &amp;amp;= W^n - \lambda \frac{\partial{L}}{\partial W}, \\
b^{n+1} &amp;amp;= b^n - \lambda \frac{\partial{L}}{\partial b}.
\end{align} $$&lt;/p>
&lt;h2 id="train-and-evaluate">Train and Evaluate&lt;/h2>
&lt;p>We now have everything in place to train the network using the training set. To recap, we initialize the weights and biases randomly and then run multiple epochs&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>, during which we get the network output (forward pass), compute the gradient of the associated loss, and update the weights and biases in the direction of the negative gradient with a constant learning rate (backpropagation). Below is all of this in code.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">input_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">output_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">784&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">10&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">epochs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">200&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">lr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">6&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">init&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">output_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">epoch&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">epochs&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">p_ai&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dW&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">db&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">gradient&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">update&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dW&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">db&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To evaluate the accuracy of the network on the training set, we compare the network&amp;rsquo;s prediction with the labels&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">accuracy&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">[];&lt;/span> &lt;span class="n">entropy_loss&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">accuracy&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">100&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">count_nonzero&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_train&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_train&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">entropy_loss&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cross_entropy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_train&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We get about %90 accuracy with this simple method on both the test and the training sets!&lt;/p>
&lt;p>
&lt;figure id="figure-final-accuracy-and-loss-after-training-for-200-steps">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Final result of training" srcset="
/post/2022/learning-machine-learning/final_hu0b401a0507b8fc9c8c4856e1147d7894_127441_bb9fcc0fb6e39bc15072935c58009e4c.webp 400w,
/post/2022/learning-machine-learning/final_hu0b401a0507b8fc9c8c4856e1147d7894_127441_2dc1e69ab398c40bfcd5a110fc5ac088.webp 760w,
/post/2022/learning-machine-learning/final_hu0b401a0507b8fc9c8c4856e1147d7894_127441_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/learning-machine-learning/final_hu0b401a0507b8fc9c8c4856e1147d7894_127441_bb9fcc0fb6e39bc15072935c58009e4c.webp"
width="760"
height="380"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Final accuracy and loss after training for 200 steps.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;h2 id="wrapping-up">Wrapping up&lt;/h2>
&lt;p>The building block of a neural network is the single layer (\ref{1}). I hope you obtained an understanding of how a neural network layer is trained and what people mean when they say the machine has learned something. My goal was to provide a basic understanding of this procedure. This can get very complicated. Large operational machine learning models, such as &lt;a href="https://en.wikipedia.org/wiki/GPT-3" target="_blank" rel="noopener">GPT-3&lt;/a>, &lt;a href="https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval" target="_blank" rel="noopener">Gopher&lt;/a>, or &lt;a href="https://developer.nvidia.com/megatron-turing-natural-language-generation" target="_blank" rel="noopener">Megatron-Turing NLG&lt;/a> use many network layers with hundreds of billions of parameters, but the procedure is similar.&lt;/p>
&lt;p>You now have a few simple tools as a starting point for further inquiry. Here are a couple of directions to go from here:&lt;/p>
&lt;ul>
&lt;li>Use a more complex neural network with at least one hidden layer. The &amp;ldquo;deep&amp;rdquo; in deep learning comes from hidden layers. Each layer dramatically increases the parameter space&amp;rsquo;s dimension, which also increases computation time. But the gain in accuracy may be worth it. You can easily get about %97 accuracy with just one additional layer for the MNIST dataset.&lt;/li>
&lt;li>Batch-process your samples. Batch processing was not necessary for his example, but with higher dimensions and larger training sets, it becomes necessary. There are also indications that batch-processed (stochastic) gradient descent generalizes better.&lt;/li>
&lt;li>Use a better optimization procedure. The step size, $\lambda$, or &lt;code>lr&lt;/code>, is the most important parameter in gradient descent. I chose the rather large value of &lt;code>lr=6&lt;/code>. You can see in the plots that the loss doesn&amp;rsquo;t decrease monotonously. A simple fix for this is to multiply the learning rate with a scalar slightly less than 1, so it gets smaller at every epoch. But more importantly, you should use a better optimizer such as Adams. In fact, my interest in optimization is the reason I wrote this post. With Jingcheng Lu and Eitan Tadmor from the University of Maryland, we constructed a &lt;a href="https://anilzen.github.io/publication/2022-swarm-based-gradient-descent/">swarm-based gradient descent&lt;/a> that avoids getting trapped at local minima. The method beats existing optimizers in certain types of non-convex optimization problems. I&amp;rsquo;m experimenting to see whether it&amp;rsquo;s useful in machine learning applications.&lt;/li>
&lt;/ul>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>NumPy is a free, open-source Python library for scientific computing and data analysis. It has a lot of functionality, but the main reason for using NumPy here is its speed. It uses vectorization instead of looping through individual elements to perform calculations on an array, allowing faster calculations and more efficient use of memory. NumPy operations are implemented in C using highly optimized libraries that take advantage of modern processor architectures, so we are not slowed down by Python&amp;rsquo;s excruciatingly inefficient loops.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>This is common practice in theoretical physics, in particular, in general relativity, where we have maps between and projections onto various different spaces and subspaces. The tensor notation using different types of letters helps keep track of the spaces in the computations. I was tempted to introduce the Einstein summation convention here as well, but it doesn&amp;rsquo;t quite work.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>You might recognize this expression of entropy. It appears commonly as $-\sum p_i \log p_i$ in different entropy formulas such as Gibbs entropy, Shannon entropy, and von Neumann entropy. In our case, it&amp;rsquo;s a way of measuring the difference between two probability distributions. We compare the true distribution of the labeled data, $y$, and the predicted probability distribution from the model, $p$. When the same probability distribution $p$ is used on either side of the log function, for example, in Shannon entropy, it measures the amount of uncertainty or randomness in that given probability distribution, which is useful for encoding information or measuring the level of disorder in a system. The purpose and interpretation are different in those cases, but the underlying similarity is the quantification of disorder, which justifies using the term entropy.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>It seems that in machine learning, NumPy, and many other mathematical software programs, $\log$ refers to the natural logarithm with base $e$, which I adopted for this post. It is confusing because, in information theory, $\log$ commonly refers to base 2. But of course &lt;a href="https://en.wikipedia.org/wiki/Logarithm#Particular_bases" target="_blank" rel="noopener">everybody knows that&lt;/a> $\log$ is base 10, and for the other stuff, you either write $\ln$ for base $e$ or $\log_2$ for base 2. In short, use your $\log$ with caution.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>You might think that the double transpose in &lt;code>np.dot(dL.T,x).T&lt;/code> is unnecessary. Why not write it as &lt;code>np.dot(x.T, dL)&lt;/code>? It turns out that the former dot product is faster because of the way the data is stored in memory.&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>These are actually iterations. The term epochs alludes to a bright future where you might want to process your training data in batches. Batch processing is among the many directions you might want to expand the code.&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Swarm-Based Gradient Descent Method for Non-Convex Optimization</title><link>https://anilzen.github.io/publication/2022-swarm-based-gradient-descent/</link><pubDate>Tue, 01 Nov 2022 16:52:40 -0500</pubDate><guid>https://anilzen.github.io/publication/2022-swarm-based-gradient-descent/</guid><description/></item><item><title>How to draw Penrose diagrams</title><link>https://anilzen.github.io/post/2022/drawing-penrose-diagrams/</link><pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/post/2022/drawing-penrose-diagrams/</guid><description>&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Penrose_diagram" target="_blank" rel="noopener">Penrose diagram&lt;/a> is a valuable tool in relativity to illustrate the global causal structure of spacetimes&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Often, qualitative diagrams are sufficient if you&amp;rsquo;re mainly interested in the essential causal relationships. But sometimes, for numerical work or for your own understanding, you need a quantitatively correct diagram.&lt;/p>
&lt;p>Below, we draw Penrose diagrams using free and open-source software. We&amp;rsquo;ll draw the diagrams using the LaTeX package &lt;a href="https://en.wikipedia.org/wiki/PGF/TikZ" target="_blank" rel="noopener">TikZ&lt;/a> (not a drawing program but a program that draws&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>). We&amp;rsquo;ll use Python to perform transformations when we hit the computational limitations of TikZ.&lt;/p>
&lt;div class="alert alert-note">
&lt;div>
The code for all diagrams below is on my &lt;a href="https://github.com/anilzen/anilzen.github.io/tree/main/content/post/2022/drawing-penrose-diagrams/tikz" target="_blank" rel="noopener">website repository&lt;/a>.
&lt;/div>
&lt;/div>
&lt;h3 id="the-basic-idea-behind-penrose-diagrams">The basic idea behind Penrose diagrams&lt;/h3>
&lt;p>The Penrose diagram is an extension, or better, a &lt;em>completion&lt;/em> of the &lt;a href="https://en.wikipedia.org/wiki/Spacetime_diagram" target="_blank" rel="noopener">Minkowski diagram&lt;/a>. Like in the Minkowski diagram, time is vertical, space is horizontal, and null rays are at 45 degrees to the axes. In contrast to the Minkowski diagram, a Penrose diagram includes &amp;ldquo;points at infinity&amp;rdquo; added by compactification, thereby visualizing the rich structure of infinity arising from the unification of space and time.&lt;/p>
&lt;p>Below are two beautiful &lt;a href="https://tikz.net/relativity_penrose_diagram/" target="_blank" rel="noopener">TikZ diagrams&lt;/a> by &lt;a href="https://www.physik.uzh.ch/en/researcharea/cms/people/Izaak-Neutelings.html" target="_blank" rel="noopener">Izaak Neutelings&lt;/a>. On the left is the Minkowski diagram. Spacetime extends infinitely in all directions. On the right is the Penrose diagram representing the entire spacetime in a finite square.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;/th>
&lt;th style="text-align:center">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure id="figure-minkowski-diagramhttpstikznetrelativity_penrose_diagram">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="[Minkowski diagram](https://tikz.net/relativity_penrose_diagram/)" srcset="
/post/2022/drawing-penrose-diagrams/figures/neutelings_minkowski_hub48f7d8ad14fbf2ddf097172b8405b3c_351951_00daccc019c1c8e7d5b53f79e0e59c99.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/neutelings_minkowski_hub48f7d8ad14fbf2ddf097172b8405b3c_351951_463aa2ce8f62b82dc39936265db730ef.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/neutelings_minkowski_hub48f7d8ad14fbf2ddf097172b8405b3c_351951_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/neutelings_minkowski_hub48f7d8ad14fbf2ddf097172b8405b3c_351951_00daccc019c1c8e7d5b53f79e0e59c99.webp"
width="760"
height="626"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;a href="https://tikz.net/relativity_penrose_diagram/" target="_blank" rel="noopener">Minkowski diagram&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure id="figure-penrose-diagramhttpstikznetrelativity_penrose_diagram">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="[Penrose Diagram](https://tikz.net/relativity_penrose_diagram/)" srcset="
/post/2022/drawing-penrose-diagrams/figures/neutelings_penrose_hud64c81516ee3adbf604a4e1198a47bf6_845479_ac06b1fd002695e2e4dc0081a38f765b.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/neutelings_penrose_hud64c81516ee3adbf604a4e1198a47bf6_845479_6a8d1b9f9b8453134e80447669cc1564.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/neutelings_penrose_hud64c81516ee3adbf604a4e1198a47bf6_845479_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/neutelings_penrose_hud64c81516ee3adbf604a4e1198a47bf6_845479_ac06b1fd002695e2e4dc0081a38f765b.webp"
width="760"
height="661"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;a href="https://tikz.net/relativity_penrose_diagram/" target="_blank" rel="noopener">Penrose Diagram&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Compactification maps the Minkowski diagram to the Penrose diagram by mapping the null directions to a finite interval. Let&amp;rsquo;s see how that works.&lt;/p>
&lt;h3 id="minkowski-spacetime">Minkowski spacetime&lt;/h3>
&lt;p>Consider the 4-dimensional Minkowski spacetime with standard spherical coordinates: $t \in (-\infty,\infty)$, $r \in [0,\infty)$, $\theta\in[0,\pi]$, $\varphi\in[0,2\pi)$ with metric
$$ ds^2 = -dt^2 + dr^2 + r^2 d\sigma^2, $$
where $d\sigma^2=d\theta^2+\sin^2\theta d\varphi^2$. Penrose diagrams are two-dimensional, so we essentially ignore the angular coordinates represented by the $d\sigma^2$ piece. Each point on the diagram, except the line at the origin, represents a sphere. Spherical light rays propagate along the directions $t+r$ and $t-r$. These directions are called &lt;a href="https://en.wikipedia.org/wiki/Null_vector" target="_blank" rel="noopener">null&lt;/a> because they are in the &lt;a href="https://en.wikipedia.org/wiki/Kernel_%28linear_algebra%29" target="_blank" rel="noopener">nullspace&lt;/a> of the metric.&lt;/p>
&lt;p>Underlying the Penrose diagrams are coordinates, $T$ and $R$, which compactify these null directions:
$$ T+R = \frac{2}{\pi}\textcolor{DarkOrchid}{\arctan}(t+r),$$
$$ T-R = \frac{2}{\pi}\textcolor{DarkOrchid}{\arctan}(t-r). $$
The scale factor $2/\pi$ is to map the range of the coordinates to $T\pm R \in(-1,1)$. Note that I&amp;rsquo;m not including the endpoints in the intervals yet. The Penrose compactification procedure involves the conformal completion of the spacetime, which adds the points at infinity after a regularizing rescaling. This detail is not relevant for the diagrams but is important for geometry.&lt;/p>
&lt;p>The Penrose coordinates $(T,R)$ read in terms of the standard $(t,r)$ as:
$$ T(t,r) = \frac{1}{\pi}\left( \arctan(t+r) + \arctan(t-r) \right)$$
$$ R(t,r) = \frac{1}{\pi}\left( \arctan(t+r) - \arctan(t-r) \right)$$&lt;/p>
&lt;p>Let&amp;rsquo;s make some plots using these coordinates. I&amp;rsquo;ll demonstrate plotting time surfaces (level sets of a time coordinate). Say you&amp;rsquo;d like to draw constant $t$ surfaces in the Penrose diagram. You might be tempted to solve the above relationships for a graph function $T(R;t)$ with constant $t$. But it is easier to let the machine do the work and plot the surfaces parametrically. Define the functions $T(t,r)$ and $R(t,r)$ in TikZ as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-latex" data-lang="latex">&lt;span class="line">&lt;span class="cl">&lt;span class="k">\tikzset&lt;/span>&lt;span class="nb">{&lt;/span>declare function=&lt;span class="nb">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> T(&lt;span class="k">\t&lt;/span>,&lt;span class="k">\r&lt;/span>) = &lt;span class="k">\fpeval&lt;/span>&lt;span class="nb">{&lt;/span>1/pi*(atan(&lt;span class="k">\t&lt;/span>+&lt;span class="k">\r&lt;/span>) + atan(&lt;span class="k">\t&lt;/span>-&lt;span class="k">\r&lt;/span>))&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> R(&lt;span class="k">\t&lt;/span>,&lt;span class="k">\r&lt;/span>) = &lt;span class="k">\fpeval&lt;/span>&lt;span class="nb">{&lt;/span>1/pi*(atan(&lt;span class="k">\t&lt;/span>+&lt;span class="k">\r&lt;/span>) - atan(&lt;span class="k">\t&lt;/span>-&lt;span class="k">\r&lt;/span>))&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">}}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then, plot the lines parametrically.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-latex" data-lang="latex">&lt;span class="line">&lt;span class="cl">&lt;span class="k">\def\Nlines&lt;/span>&lt;span class="nb">{&lt;/span>6&lt;span class="nb">}&lt;/span> &lt;span class="c">% total number of lines is 2\Nlines+1
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span>&lt;span class="k">\newcommand\samp&lt;/span>&lt;span class="na">[1]&lt;/span>&lt;span class="nb">{&lt;/span> tan(90*#1) &lt;span class="nb">}&lt;/span> &lt;span class="c">% for equidistant sampling
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span>&lt;span class="k">\foreach&lt;/span> &lt;span class="k">\i&lt;/span> [evaluate=&lt;span class="nb">{&lt;/span>&lt;span class="k">\t&lt;/span>=&lt;span class="k">\i&lt;/span>/(&lt;span class="k">\Nlines&lt;/span>+1);&lt;span class="nb">}&lt;/span>] in &lt;span class="nb">{&lt;/span>-&lt;span class="k">\Nlines&lt;/span>,...,&lt;span class="k">\Nlines&lt;/span>&lt;span class="nb">}{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\message&lt;/span>&lt;span class="nb">{&lt;/span>Drawing i=&lt;span class="k">\i&lt;/span>...&lt;span class="nb">^^&lt;/span>J&lt;span class="nb">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\draw&lt;/span>&lt;span class="na">[line width=0.3,samples=30,smooth,variable=\r,domain=0.001:1]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> plot(&lt;span class="nb">{&lt;/span> R(&lt;span class="k">\samp&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="k">\t&lt;/span>&lt;span class="nb">}&lt;/span>,&lt;span class="k">\samp&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="k">\r&lt;/span>&lt;span class="nb">}&lt;/span>) &lt;span class="nb">}&lt;/span>, &lt;span class="nb">{&lt;/span> T(&lt;span class="k">\samp&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="k">\t&lt;/span>&lt;span class="nb">}&lt;/span>,&lt;span class="k">\samp&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="k">\r&lt;/span>&lt;span class="nb">}&lt;/span>) &lt;span class="nb">}&lt;/span>);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We&amp;rsquo;re sampling points fairly evenly by incorporating the compactification into the plot function. As you can see in the diagrams below, the time surfaces are equally separated from each other at the origin. The function &lt;code>samp&lt;/code> controls the separation of points in the plot.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;/th>
&lt;th style="text-align:center">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure id="figure-compactification-with-arctan-filetikzminkmink_arctantex">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Compactification with arctan [(file)](tikz/mink/mink_arctan.tex)" srcset="
/post/2022/drawing-penrose-diagrams/figures/mink_arctan_hu0ed202cf971a20ea79ed5ddc7d6a4c2f_63882_7c87c5b70173cea05b03cb6ef62b45e7.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/mink_arctan_hu0ed202cf971a20ea79ed5ddc7d6a4c2f_63882_4eb7334e9c0a75b4d551c2668a6b69ba.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/mink_arctan_hu0ed202cf971a20ea79ed5ddc7d6a4c2f_63882_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/mink_arctan_hu0ed202cf971a20ea79ed5ddc7d6a4c2f_63882_7c87c5b70173cea05b03cb6ef62b45e7.webp"
width="438"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Compactification with arctan &lt;a href="tikz/mink/mink_arctan.tex">(file)&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure id="figure-compactification-with-tanh-filetikzminkmink_tanhtex">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Compactification with tanh [(file)](tikz/mink/mink_tanh.tex)" srcset="
/post/2022/drawing-penrose-diagrams/figures/mink_tanh_hu39a38b3c1d47f28d8ceb9cd1fe0fce12_61420_843cb384a6441847b2660c6b3d4b8f27.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/mink_tanh_hu39a38b3c1d47f28d8ceb9cd1fe0fce12_61420_b04b00b0785f30a374e685c6722f9527.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/mink_tanh_hu39a38b3c1d47f28d8ceb9cd1fe0fce12_61420_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/mink_tanh_hu39a38b3c1d47f28d8ceb9cd1fe0fce12_61420_843cb384a6441847b2660c6b3d4b8f27.webp"
width="438"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Compactification with tanh &lt;a href="tikz/mink/mink_tanh.tex">(file)&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The two versions of the diagram illustrate a coordinate-dependent feature that confuses even the experts. The $t$ surfaces intersect at spatial infinity, $i^0$, on both diagrams. On the left, they are tangent to each other, while on the right, they are not. So this seems to be a coordinate-dependent feature. Penrose used the inverse tangent function in his original papers. Many other choices exist. For example, using the hyperbolic tangent, the mapping reads
$$ T+R = \textcolor{DarkOrchid}{\tanh}(t+r),$$
$$ T-R = \textcolor{DarkOrchid}{\tanh}(t-r). $$
This mapping gives the diagram on the right where $t$ surfaces are not tangential at spatial infinity. There&amp;rsquo;s a subtlety here. Conformal compactification with hyperbolic tangent is not regular when you include the angular dimensions. So be careful what you choose for the mapping.&lt;/p>
&lt;p>I draw Penrose diagrams typically to present the causal structure of &lt;a href="https://hyperboloid.al" target="_blank" rel="noopener">hyperboloidal&lt;/a> surfaces. Such surfaces behave like spacetime hyperboloids: they are spacelike everywhere, including null horizons. Spacetime hyperboloids are typically defined by the level sets of $\tau$ as
$$ t^2 - r^2 = \tau^2 \implies \tau = \pm \sqrt{t^2 - r^2}. $$
This construction appears in many models, such as the &lt;a href="https://en.wikipedia.org/wiki/Milne_model" target="_blank" rel="noopener">Milne model&lt;/a> of cosmology, &lt;a href="https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.21.392" target="_blank" rel="noopener">Dirac&amp;rsquo;s point-form&lt;/a> of relativistic dynamics, the &lt;a href="https://arxiv.org/abs/hep-th/0303006" target="_blank" rel="noopener">de Boer-Solodukhin&lt;/a> holography of Minkowski spacetime, and the &lt;a href="https://arxiv.org/abs/1304.2794" target="_blank" rel="noopener">Buchholz-Roberts framework&lt;/a> of relativistic QED. However, these surfaces are not generally useful for studying evolution in time because they intersect at future null infinity. This may not be immediately obvious from their definition, but you can see it right away on the Penrose diagram (left panel below).&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;/th>
&lt;th style="text-align:center">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure id="figure-intersecting-hyperbolic-slicing-filetikzminkmink_hypal_intersecttex">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Intersecting hyperbolic slicing [(file)](./tikz/mink/mink_hypal_intersect.tex)" srcset="
/post/2022/drawing-penrose-diagrams/figures/mink_hypal_intersect_hu7747645cf07242a4588871dffffb0c08_48436_5332b674093f643b42a9357e1afcba7a.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/mink_hypal_intersect_hu7747645cf07242a4588871dffffb0c08_48436_85dcb9c9c06ca82a3c6b36b23a9bc33a.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/mink_hypal_intersect_hu7747645cf07242a4588871dffffb0c08_48436_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/mink_hypal_intersect_hu7747645cf07242a4588871dffffb0c08_48436_5332b674093f643b42a9357e1afcba7a.webp"
width="440"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Intersecting hyperbolic slicing &lt;a href="./tikz/mink/mink_hypal_intersect.tex">(file)&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure id="figure-smooth-hyperboloidal-foliation-filetikzminkmink_hypal_foliationtex">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Smooth hyperboloidal foliation [(file)](tikz/mink/mink_hypal_foliation.tex)" srcset="
/post/2022/drawing-penrose-diagrams/figures/mink_hypal_foliation_hu3f5b5c63b3b9a07f4845771a0daebf93_45539_18631d5485c16d6c7c2bc7be7aaf3eb5.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/mink_hypal_foliation_hu3f5b5c63b3b9a07f4845771a0daebf93_45539_a4bae7e0f37eb8922bfaa8622f12bf45.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/mink_hypal_foliation_hu3f5b5c63b3b9a07f4845771a0daebf93_45539_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/mink_hypal_foliation_hu3f5b5c63b3b9a07f4845771a0daebf93_45539_18631d5485c16d6c7c2bc7be7aaf3eb5.webp"
width="457"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Smooth hyperboloidal foliation &lt;a href="tikz/mink/mink_hypal_foliation.tex">(file)&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>A better option for a foliation of Minkowski spacetime, illustrated on the right panel above, is to use time-shifted hyperboloids. We pick one spacetime hyperboloid, say, with unit radius, and shift it in time by $\tau$, like this
$$ (t-\tau)^2 - r^2 = 1 \implies \tau = t \pm \sqrt{1+r^2}. $$
Choosing the minus sign gives a future hyperboloidal foliation. The surfaces do not intersect but provide a smooth foliation of future null infinity.&lt;/p>
&lt;p>This example demonstrates how causal structures that may not be immediately clear from the formulas can be illustrated and understood with Penrose diagrams. Other examples include the counterintuitive causal structure of hyperboloidal surfaces or the intersection of standard time surfaces at spatial infinity. We can demonstrate such properties by calculations, but it&amp;rsquo;s much easier to understand them with the help of a Penrose diagram.&lt;/p>
&lt;h4 id="reading-data-into-tikz">Reading data into TikZ&lt;/h4>
&lt;p>TikZ is limited in its data processing capabilities. As a consequence, the evaluation of the lines in the plots takes a while. Once you move away from Minkowski spacetime, the transformations become too complicated for TikZ. To handle complicated mathematical transformations, you can generate the data for the plots in Python, write them in a file, and plot them with TikZ. Below is the Python code to generate the data files for each time surface.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linspace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mf">2.&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">14&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">endpoint&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">r&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linspace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">30&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">penrose_coords&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">R&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">T&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">T&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">t_val&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">enumerate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">R&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">T&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">penrose_coords&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t_val&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">savetxt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;arctan_data&amp;#39;&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="s1">&amp;#39;.csv&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">R&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">delimiter&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;,&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">fmt&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span>&lt;span class="si">%f&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">header&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;R,T&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">comments&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You&amp;rsquo;ll need the &lt;a href="https://numpy.org/" target="_blank" rel="noopener">numpy&lt;/a> library installed in your environment (use &lt;a href="https://xkcd.com/1987/" target="_blank" rel="noopener">local environments for Python!&lt;/a>). The array &lt;code>t&lt;/code> contains the constant values of the time surfaces, the array &lt;code>r&lt;/code> includes the samples of the plot parameter. The domains stop at $\pi/2$ because we use the $\tan$ function to sample the points for a relatively even distribution. Then, for each value of &lt;code>t&lt;/code>, we write the $R,T$ coordinates into a CSV file with headers and some formatting.&lt;/p>
&lt;p>On the TikZ side, we read these points from the respective files and plot them. To plot data points, I use the library &lt;code>pgfplots&lt;/code>. The nodes must be drawn with respect to the axis of the plot, which is achieved with the &lt;code>axis cs:&lt;/code> directive. The relevant part of the code is below.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-latex" data-lang="latex">&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\begin&lt;/span>&lt;span class="nb">{&lt;/span>axis&lt;span class="nb">}&lt;/span>[axis lines=none, xmin=-.1,xmax=1.1,ymin=-1.2,ymax=1.2,width=0.5&lt;span class="k">\textwidth&lt;/span>,height=0.8&lt;span class="k">\textwidth&lt;/span>]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\node&lt;/span>&lt;span class="na">[right]&lt;/span> at (axis cs:0.6,1) &lt;span class="nb">{&lt;/span>&lt;span class="k">\small&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nv">\arctan&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="nv">\cdot&lt;/span>&lt;span class="o">)&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\coordinate&lt;/span> (O) at (axis cs:0,0) ; &lt;span class="c">% center: origin (r,t) = (0,0)
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span> &lt;span class="k">\coordinate&lt;/span> (S) at ( axis cs:0,-1); &lt;span class="c">% south: t=-infty, i-
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span> &lt;span class="k">\coordinate&lt;/span> (N) at ( axis cs:0, 1); &lt;span class="c">% north: t=+infty, i+
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span> &lt;span class="k">\coordinate&lt;/span> (E) at ( axis cs:1, 0); &lt;span class="c">% east: r=+infty, i0
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span> &lt;span class="k">\draw&lt;/span>&lt;span class="na">[thick]&lt;/span> (N) -- (E) -- (S) -- cycle;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\newcommand&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="k">\scri&lt;/span>&lt;span class="nb">}{&lt;/span>&lt;span class="k">\mathscr&lt;/span>&lt;span class="nb">{&lt;/span>I&lt;span class="nb">}}&lt;/span> &lt;span class="c">% null infinity
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span> &lt;span class="k">\node&lt;/span>&lt;span class="na">[right]&lt;/span> at (E) &lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">i^&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\node&lt;/span>&lt;span class="na">[above]&lt;/span> at (N) &lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">i^&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\node&lt;/span>&lt;span class="na">[below]&lt;/span> at (S) &lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">i^&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\node&lt;/span>&lt;span class="na">[above, rotate=90]&lt;/span> at (O) &lt;span class="nb">{&lt;/span>&lt;span class="k">\small&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">r&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\node&lt;/span>&lt;span class="na">[above right]&lt;/span> at (axis cs: 0.5,0.5) &lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nv">\scri&lt;/span>&lt;span class="nb">^&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\node&lt;/span>&lt;span class="na">[below right]&lt;/span> at (axis cs: 0.5,-0.5) &lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nv">\scri&lt;/span>&lt;span class="nb">^&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\foreach&lt;/span> &lt;span class="k">\file&lt;/span> in &lt;span class="nb">{{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data0.csv&lt;span class="nb">}&lt;/span>,&lt;span class="nb">{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data1.csv&lt;span class="nb">}&lt;/span>,&lt;span class="nb">{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data2.csv&lt;span class="nb">}&lt;/span>,&lt;span class="nb">{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data3.csv&lt;span class="nb">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data4.csv&lt;span class="nb">}&lt;/span>,&lt;span class="nb">{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data5.csv&lt;span class="nb">}&lt;/span>,&lt;span class="nb">{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data6.csv&lt;span class="nb">}}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">{&lt;/span>&lt;span class="k">\addplot&lt;/span>&lt;span class="na">[domain={-1,1}]&lt;/span> table [x=R, y=T, col sep=comma] &lt;span class="nb">{&lt;/span>&lt;span class="k">\file&lt;/span>&lt;span class="nb">}&lt;/span>;&lt;span class="nb">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">\end&lt;/span>&lt;span class="nb">{&lt;/span>axis&lt;span class="nb">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This is probably not the most elegant solution; you switch platforms to draw the plots. If you&amp;rsquo;re up for it, you might prefer using a Python library such as &lt;a href="https://pypi.org/project/tikzplotlib/" target="_blank" rel="noopener">tikzplotlib&lt;/a> to avoid going back and forth between Python and TikZ.&lt;/p>
&lt;h3 id="schwarzschild-spacetime">Schwarzschild spacetime&lt;/h3>
&lt;p>The Schwarzschild line element in standard (Droste) coordinates reads
$$ ds^2 = - f(r) dt^2 + \frac{1}{f(r)} dr^2 + r^2 d\sigma^2, \tag{SS} \label{1}$$
where
$$ f(r) = 1-\frac{2 M}{r},$$
or in dimensionless coordinates
$$ f(r) = 1-\frac{1}{r}.$$
Dimensionless coordinates are similar to setting $M=1/2$.&lt;/p>
&lt;p>The metric is singular where $f(r)$ vanishes, at $r=1$. This surface is the &lt;a href="https://en.wikipedia.org/wiki/Event_horizon" target="_blank" rel="noopener">event horizon&lt;/a>, but the metric singularity is a coordinate artifact. There are regular coordinates across the event horizon, such as the &lt;a href="https://en.wikipedia.org/wiki/Gullstrand%E2%80%93Painlev%C3%A9_coordinates" target="_blank" rel="noopener">Gullstrand–Painlevé&lt;/a> or the &lt;a href="https://en.wikipedia.org/wiki/Eddington%E2%80%93Finkelstein_coordinates" target="_blank" rel="noopener">Eddington–Finkelstein&lt;/a> coordinates. In fact, we must use such regular coordinates to draw Penrose diagrams.&lt;/p>
&lt;p>Penrose diagrams for Schwarzschild spacetime are traditionally drawn using a compactification of &lt;a href="https://en.wikipedia.org/wiki/Kruskal%E2%80%93Szekeres_coordinates" target="_blank" rel="noopener">Kruskal coordinates&lt;/a>. Let&amp;rsquo;s copy them from Wikipedia (for a derivation, see, for example, the Appendix of my &lt;a href="https://anilzen.github.io/publication/zenginoglu-2007-conformal/" target="_blank" rel="noopener">thesis&lt;/a>):
$$ \tau = (r-1) e^r \sinh \tfrac{t}{2}, $$
$$ \rho = (r-1) e^r \cosh \tfrac{t}{2}. $$
The coordinates of the Penrose diagram are compactified along the null directions just as in the Minkowski case:
$$ T = \frac{1}{\pi}\left( \arctan(\tau+\rho) + \arctan(\tau-\rho) \right)$$
$$ R = \frac{1}{\pi}\left( \arctan(\tau+\rho) - \arctan(\tau-\rho) \right)$$
The transformations are more complicated, so we will use Python codes to generate the lines and plot them with TikZ. Here&amp;rsquo;s the Python function to construct the compactified Kruskal coordinates.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">kruskal_coords&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">t&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">rho&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cosh&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tau&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sinh&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">R&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">2.&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tau&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">rho&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tau&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">rho&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">T&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">2.&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tau&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">rho&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tau&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">rho&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">T&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>I construct Schwarzschild radial points from tortoise coordinates that are on Chebyshev nodes because I want higher density near the horizon and infinity. There are different, possibly more effective ways of achieving such a beneficial point distribution, but this method works well enough.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">t_vals&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linspace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">3.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">7&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">r_tort&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cos&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">30&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="mi">30&lt;/span>&lt;span class="p">))[::&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">r_schw&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">real&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">lambertw&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r_tort&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">t_val&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">enumerate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t_vals&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">R&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">T&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">kruskal_coords&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r_schw&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">t_val&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">fn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;data/ss&amp;#39;&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="s1">&amp;#39;.csv&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">savetxt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">fn&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">R&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">T&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">delimiter&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;,&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">fmt&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span>&lt;span class="si">%f&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">header&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;R,T&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">comments&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;{&amp;#39;&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">fn&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="s1">&amp;#39;},&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The Schwarzschild time slices intersect at the bifurcation sphere, $\mathcal{B}$, and at spatial infinity $i^0$. When you&amp;rsquo;re interested in the behavior of fields near the black hole and far away from it (say, to study gravitational waves), it&amp;rsquo;s &lt;a href="https://anilzen.github.io/publication/zenginoglu-2011-geometric/" target="_blank" rel="noopener">better&lt;/a> to use non-intersecting time slices.&lt;/p>
&lt;p>Most useful time functions are related to the Schwarzschild time by a &amp;ldquo;height&amp;rdquo; shift that depends only on the radial coordinate:
$$ t \to t + h(r). $$
For example, the height function for &lt;a href="https://en.wikipedia.org/wiki/Gullstrand%E2%80%93Painlev%C3%A9_coordinates" target="_blank" rel="noopener">Gullstrand–Painlevé&lt;/a> coordinates is
$$ h_{\rm GP} (r) = -2 \sqrt{r} + \ln\frac{\sqrt{r}+1}{\sqrt{r}-1}. $$
Note that the height function is singular at the horizon. This singularity is needed to counteract the singularity of Schwarzschild time slices near the bifurcation sphere. Gullstrand–Painlevé gives a nice foliation of the future event horizon, $\mathcal{H}^+$, but the time slices still intersect at spatial infinity.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;/th>
&lt;th style="text-align:center">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure id="figure-schwarzschild-droste-time-slices">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Schwarzschild-Droste time slices" srcset="
/post/2022/drawing-penrose-diagrams/figures/ss_standard_hu727494f74e41712379bdfed7d71bfe07_305624_e40ae1b1bcf9878a0ebd29d8891af4da.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/ss_standard_hu727494f74e41712379bdfed7d71bfe07_305624_ac19cab7f13834bfd457c9df7e2072af.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/ss_standard_hu727494f74e41712379bdfed7d71bfe07_305624_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/ss_standard_hu727494f74e41712379bdfed7d71bfe07_305624_e40ae1b1bcf9878a0ebd29d8891af4da.webp"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Schwarzschild-Droste time slices
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure id="figure-gullstrandpainlevé-time-slices">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Gullstrand–Painlevé time slices" srcset="
/post/2022/drawing-penrose-diagrams/figures/ss_gp_huc45694372c238d974acb5d587f3bbeb8_320514_627235e3a36de0231d5f308663c9288a.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/ss_gp_huc45694372c238d974acb5d587f3bbeb8_320514_e6bc86a8e8efe39e055b87f72c278f9b.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/ss_gp_huc45694372c238d974acb5d587f3bbeb8_320514_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/ss_gp_huc45694372c238d974acb5d587f3bbeb8_320514_627235e3a36de0231d5f308663c9288a.webp"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Gullstrand–Painlevé time slices
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>We need a height function that&amp;rsquo;s singular both near and far from the black hole. This idea underlies the construction of hyperboloidal coordinates. For example,
$$ h_{\rm Hyp} (r) = \sqrt{1+ r_\ast^2}, $$
gives the hyperboloidal foliation on the left panel below. The slices do not intersect. Instead, you get a nice, smooth foliation of the full exterior domain. Another example with this nice behavior is the minimal gauge of &lt;a href="https://arxiv.org/abs/1809.02837" target="_blank" rel="noopener">Ansorg, Jaramillo, and Macedo&lt;/a>
$$ h_{\rm MG} (r) = r + 2 \ln r - \ln(r - 1). $$&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;/th>
&lt;th style="text-align:center">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure id="figure-hyperbolic-time-slices">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Hyperbolic time slices" srcset="
/post/2022/drawing-penrose-diagrams/figures/ss_hyperbolic_hu727494f74e41712379bdfed7d71bfe07_267505_619c70a07d8ef7fd1fa8eac3ec505535.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/ss_hyperbolic_hu727494f74e41712379bdfed7d71bfe07_267505_cf6b5d0752b76d74c305fe585553582f.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/ss_hyperbolic_hu727494f74e41712379bdfed7d71bfe07_267505_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/ss_hyperbolic_hu727494f74e41712379bdfed7d71bfe07_267505_619c70a07d8ef7fd1fa8eac3ec505535.webp"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Hyperbolic time slices
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure id="figure-minimal-gauge">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Minimal gauge" srcset="
/post/2022/drawing-penrose-diagrams/figures/ss_minimal_hu727494f74e41712379bdfed7d71bfe07_260868_edd5824f8e851782d2c357d381a0ccf6.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/ss_minimal_hu727494f74e41712379bdfed7d71bfe07_260868_c34845ff30f7a4ae6610769aa013eeb2.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/ss_minimal_hu727494f74e41712379bdfed7d71bfe07_260868_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/ss_minimal_hu727494f74e41712379bdfed7d71bfe07_260868_edd5824f8e851782d2c357d381a0ccf6.webp"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Minimal gauge
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Similar constructions can also be made for Kerr, Reissner–Nordström, or Schwarzschild-de Sitter metrics.&lt;/p>
&lt;p>That&amp;rsquo;s it! I hope this helps for the handful of people out there who draw Penrose diagrams. If you have any questions about the diagrams or the source files, email me at &lt;a href="mailto:anil@umd.edu">anil@umd.edu&lt;/a>. I plan to write two more posts on Penrose compactification that should be of broader interest. Stay tuned!&lt;/p>
&lt;!-- These coordinates are good for plotting the maximally extended Schwarzschild solution but they are too complicated if you're primarily interested in the exterior region. You can draw a Penrose diagram for Schwarzschild just like in the Minkowski case by using the tortoise coordinate. The reason is that the $t,r$ portion of the Schwarzschild metric is conformally flat in the tortoise coordinate. Since Penrose diagrams are conformal diagrams, all the usual tricks apply. -->
&lt;!-- The Schwarzschild case reduces to the Minkowski case when we switch to the tortoise coordinate (up to a constant)
$$ r_\ast = \int \frac{dr}{f(r)} = r + 2 M \log \left(\frac{r}{2M}-1\right).$$
The Schwarzschild metric \eqref{1} becomes
$$ ds^2 = f(r(r_\ast)) (- dt^2 + dr_\ast^2) + r(r_\ast)^2 d\sigma^2. $$
Ignoring the angular part, the spacetime metric is just a conformal Minkowski metric in coordinates $(t,r_\ast)$ with in- and outgoing coordinates $t+r_\ast$ and $t-r_\ast$. For a Penrose diagram, you can consider the Schwarzschild metric to be simply $ -dt^2 + dr_\ast^2$. The main difference is the domain of $r_\ast$. While the domain of $r$ in Minkowski is $[0,\infty)$, the domain of the tortoise coordinate $r_\ast$ in Schwarzschild is $(-\infty, \infty)$. This difference is why the Minkowski conformal diagram is a triangle, whereas the conformal diagram for the exterior Schwarzschild spacetime is a diamond. -->
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>For a rigorous, mathematical definition of Penrose diagrams, see the Appendix C.2 in a &lt;a href="https://arxiv.org/abs/gr-qc/0309115" target="_blank" rel="noopener">paper&lt;/a> by Dafermos and Rodnianski.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>The curious capitalization of TikZ derives from the capitalization of the German expression &amp;ldquo;TikZ is kein Zeichenprogramm.&amp;rdquo;&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Hyperboloidal method for frequency-domain self-force calculations</title><link>https://anilzen.github.io/publication/macedo-2022-self-force/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/macedo-2022-self-force/</guid><description/></item><item><title>Jealous of hyperbolic fish</title><link>https://anilzen.github.io/post/2022/jealous-of-fish/</link><pubDate>Sun, 23 Jan 2022 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/post/2022/jealous-of-fish/</guid><description>&lt;p>Inspired by the following tweet from the great &lt;a href="https://www.quantamagazine.org/" target="_blank" rel="noopener">Quanta Magazine&lt;/a>.
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">Physicists are jealous of these fish drawn by M.C. Escher. If we lived in a world with a geometry like theirs, we may have a much simpler time formulating theories about the quantum nature of gravity. &lt;a href="https://t.co/RuZFkM4pCO">https://t.co/RuZFkM4pCO&lt;/a> &lt;a href="https://t.co/2LAHKuGW8C">pic.twitter.com/2LAHKuGW8C&lt;/a>&lt;/p>&amp;mdash; Quanta Magazine (@QuantaMagazine) &lt;a href="https://twitter.com/QuantaMagazine/status/1478861128553971718?ref_src=twsrc%5Etfw">January 5, 2022&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;/p>
&lt;h3 id="kusturica-against-wallace">Kusturica against Wallace&lt;/h3>
&lt;p>The featured image on the tweet above shows &lt;a href="https://en.wikipedia.org/wiki/M._C._Escher" target="_blank" rel="noopener">Escher&lt;/a>&amp;rsquo;s &lt;a href="https://en.wikipedia.org/wiki/Circle_Limit_III" target="_blank" rel="noopener">Circle Limit III&lt;/a> depicting the Poincaré model of hyperbolic geometry. Escher was interested in constructing an infinitely repeating pattern in a finite figure. When the brilliant geometer &lt;a href="https://en.wikipedia.org/wiki/Harold_Scott_MacDonald_Coxeter" target="_blank" rel="noopener">Coxeter&lt;/a> shared with him a reprint of one of his lectures on hyperbolic geometry, Escher figured out how to create such &lt;a href="https://en.wikipedia.org/wiki/Tessellation" target="_blank" rel="noopener">tessalations&lt;/a>. The &lt;a href="http://www.ams.org/publicoutreach/feature-column/fcarc-circle-limit" target="_blank" rel="noopener">story&lt;/a> of how Escher managed to create these &lt;a href="https://mathstat.slu.edu/escher/index.php/Circle_Limit_Exploration" target="_blank" rel="noopener">four woodcuts&lt;/a> is fascinating.&lt;/p>
&lt;p>The original &lt;a href="https://www.quantamagazine.org/cosmologists-close-in-on-logical-laws-for-the-big-bang-20211110/" target="_blank" rel="noopener">Quanta article&lt;/a> in the tweet above reports the difficulties in constructing something like the &lt;a href="https://en.wikipedia.org/wiki/AdS/CFT_correspondence" target="_blank" rel="noopener">AdS/CFT correspondence&lt;/a> for physical spacetimes. While we have a realization of holographic duality based on anti-de Sitter spacetimes, a similar construction on flat or de Sitter spacetimes has been elusive, supposedly because these spacetimes do not possess hyperbolic geometry. A week after the tweet above, there was another &lt;a href="https://www.quantamagazine.org/symmetries-reveal-clues-about-the-holographic-universe-20220112/" target="_blank" rel="noopener">Quanta article&lt;/a> with hyperbolic geometry as its featured image. Clearly, there is some hype around hyperbolic geometry.&lt;/p>
&lt;p>The main argument is that spatial slices of AdS are hyperbolic, which seems to be essential for AdS/CFT, so living in hyperbolic geometry makes it easier for the fish in Escher&amp;rsquo;s drawing to know quantum gravity.&lt;/p>
&lt;blockquote>
&lt;p>The fish doesn&amp;rsquo;t think because the fish knows everything.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Emir_Kusturica" target="_blank" rel="noopener">Kusturica&lt;/a> (1993) from the movie &lt;a href="https://en.wikipedia.org/wiki/Arizona_Dream" target="_blank" rel="noopener">Arizona Dream&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>We are, however, not like those fish; we want to discover quantum gravity. The obstacle remains that our physical models for spacetime are either asymptotically flat (for isolated systems) or de Sitter (for cosmology).&lt;/p>
&lt;p>I argue below that there is no need to be jealous of the fish. We do live in hyperbolic geometry; we just don&amp;rsquo;t know it yet.&lt;/p>
&lt;blockquote>
&lt;p>There are these two young fish swimming along and they happen to meet an older fish swimming the other way, who nods at them and says &amp;ldquo;Morning, boys. How&amp;rsquo;s the water?&amp;rdquo; And the two young fish swim on for a bit, and then eventually one of them looks over at the other and goes &amp;ldquo;What the hell is water?&amp;rdquo;&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/David_Foster_Wallace" target="_blank" rel="noopener">Wallace&lt;/a> (2005) from his commencement speech &lt;a href="https://en.wikipedia.org/wiki/This_Is_Water" target="_blank" rel="noopener">This Is Water&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;h3 id="little-fish-big-fish">Little fish, big fish&lt;/h3>
&lt;p>Let&amp;rsquo;s start with a short review of the hyperbolic fish in Escher&amp;rsquo;s drawing. Hyperbolic geometry is typically introduced in a fictitious higher dimensional Minkowski space using the equation for a &lt;a href="https://en.wikipedia.org/wiki/Hyperboloid" target="_blank" rel="noopener">hyperboloid&lt;/a> in Cartesian coordinates. I&amp;rsquo;ll be using spherical coordinates foreshadowing an application below, so we have the Minkowski space with line element
$$ ds^2_{\rm{Mink}} = -dt^2 + dr^2 + r^2 d\sigma^2,$$
where $d\sigma^2=d\sigma^2+\sin^2\theta \ d\varphi^2$, the usual metric on the unit sphere.&lt;/p>
&lt;p>Spheres and hyperboloids may seem very different, but they are analogous to each other. Just like the unit sphere is the set of points at unit Euclidean distance from the origin, the unit hyperboloid is the set of points at unit Minkowski distance from the origin: the hyperboloid is simply a (pseudo)sphere of Minkowski space
$$t^2-r^2=1. \tag{1} \label{1}$$&lt;/p>
&lt;p>We already have the first hint that we live in hyperbolic geometry! The hyperboloid describes the &lt;a href="../../hyperbolic-relativity/">kinematic space of special relativity&lt;/a>. But this was rather anticlimactic so let&amp;rsquo;s go a bit further.&lt;/p>
&lt;p>We can derive and represent the intrinsic metric of hyperbolic geometry in various equivalent ways. For example, setting $t=\cosh \xi$ and $r=\sinh \xi$ satisfies the equation for the hyperboloid (\ref{1}) identically, and gives us the hyperbolic line element
$$ ds^2_{\mathbb{H}^3} = d\xi^2+ \sinh^2\xi \ d\sigma^2.$$
Note the similarity of this hyperbolic metric with the metric on the unit sphere.
Coordinates are labels and it&amp;rsquo;s helpful to see hyperbolic geometry in different representations. Instead of introducing hyperbolic functions, let&amp;rsquo;s solve (\ref{1}) directly
$$ t = \sqrt{1+r^2}. \tag{2} \label{2} $$
Replacing the $dt$ term with the differential of this relation in the Minkowski metric, we get
$$ ds^2_{\mathbb{H}^3} = \textcolor{purple}{\frac{1}{1+r^2}dr^2 + r^2 d\sigma^2}. \tag{3} \label{3} $$&lt;/p>
&lt;p>To get Escher&amp;rsquo;s image of a disk, we need to map the hyperboloid model with its infinite range in $r$ to the unit &lt;a href="https://en.wikipedia.org/wiki/Poincar%C3%A9_disk_model" target="_blank" rel="noopener">Poincaré disk model&lt;/a>. We relabel the radial points using
$$ r = \frac{2\rho}{1-\rho^2}, \tag{4}\label{4}$$
and obtain the familiar &lt;a href="https://en.wikipedia.org/wiki/Poincar%C3%A9_disk_model#Metric_and_curvature" target="_blank" rel="noopener">line element for the Poincaré ball&lt;/a>&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>
$$ ds^2_{\mathbb{H}^3} = \textcolor{darkorange}{\frac{4}{(1-\rho^2)^2}\left( d\rho^2 + \rho^2 d\sigma^2 \right)}. $$
The metric blows up near the boundary, which is how Escher packs the infinitely many fish in a repeating pattern into the disk. Remember that the fish have the same size. The Poincaré disk is a conformal model of hyperbolic geometry distorting sizes but keeping the shapes (or rather local angles) invariant, so the fish actually don&amp;rsquo;t know that they are packed so densely, contrary to claims about their omniscient nature. As far as the fish are concerned, they don&amp;rsquo;t see the distortions and might even think they live in flat space&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> 😉.&lt;/p>
&lt;h3 id="swimming-in-anti-de-sitter">Swimming in anti-de Sitter&lt;/h3>
&lt;p>The hype around hyperbolic geometry among quanta is because cuts of AdS are hyperbolic spaces. In fact, AdS is introduced very similarly to hyperbolic geometry&amp;mdash;starting with a fictitious higher dimensional Minkowski space. This Minkowski space, however, has two time dimensions (which is rather awkward): $ds^2 = -dt_1^2 - dt_2^2 + dr^2 + r^2 d\sigma^2$. We write the hyperboloid as $-t_1^2-t_2^2+r^2=-1$. This relation can be solved identically by $t_1 = \sqrt{1+r^2} \cos t$ and $t_2 = \sqrt{1+r^2} \sin t$, giving the &lt;a href="https://en.wikipedia.org/wiki/Anti-de_Sitter_space#Global_coordinates" target="_blank" rel="noopener">AdS metric in global coordinates&lt;/a>
$$ ds^2_{\rm{AdS}} = - \left(1+r^2\right) dt^2 + \textcolor{purple}{\frac{1}{1+r^2} dr^2 + r^2 d\sigma^2}.$$
AdS spacetime has many &lt;a href="https://arxiv.org/abs/1611.01118" target="_blank" rel="noopener">counterintuitive aspects&lt;/a>; it&amp;rsquo;s not even stable against small &lt;a href="https://arxiv.org/abs/1312.5544" target="_blank" rel="noopener">perturbations&lt;/a>. But you can immediately see that its $t$-slices have hyperbolic geometry by comparing them to the metric (\ref{3}), which is the crucial aspect for quantum gravity.&lt;/p>
&lt;p>I promised you to end your jealousy, but so far, I&amp;rsquo;ve just been pressing on the wound. The question remains: how do we go from our usual flat spacetime with line element
$$ ds^2_{\rm{Mink}} = -dt^2 + dr^2 + r^2 d\sigma^2$$
to the hyperbolic three-metric as time slices? Yes, we&amp;rsquo;ve encountered the kinematic space of special relativity, but that&amp;rsquo;s not a spacetime metric&amp;mdash;time is missing! Considering the usual time slices, there is nothing hyperbolic about the flat geometry of $dr^2 + r^2 d\sigma^2$.&lt;/p>
&lt;h3 id="time-heals-all-wounds">Time heals all wounds&lt;/h3>
&lt;p>In relativity, there is no notion of absolute time; we can develop our own simultaneity to connect observers and call it a proper day. Therefore, the geometry of the three-space connecting these observers is also not invariant but depends on a choice of time. Many familiar notions such as simultaneity, volume, area become unfamiliar through such reparametrizations of spacetime.&lt;/p>
&lt;p>The choice of time also determines whether the geometry of the three-space is Euclidean or hyperbolic. We are free to pick our preferred time coordinate, so let&amp;rsquo;s pick a time that makes space hyperbolic and end the jealousy.&lt;/p>
&lt;p>We&amp;rsquo;ve already seen this when we encountered the kinematic space of special relativity in our derivation of the hyperboloid model of hyperbolic geometry. Now, instead of finding intrinsic coordinates as in (\ref{2}), we introduce a new time coordinate as
$$ \tau = t - \sqrt{1+r^2}. \tag{5} \label{5} $$
The resulting metric is not a three-dimensional intrinsic spatial metric but a transformed Minkowski metric with timelike, null, and spacelike directions
$$ ds^2_{\rm{Mink}} = -d\tau^2 - \frac{2r}{\sqrt{1+r^2}} d\tau dr + \textcolor{purple}{\frac{1}{1+r^2} dr^2 + r^2 d\sigma^2}.$$
Voilà! The $\tau$ slices have hyperbolic geometry. Relabeling the radial points as in (\ref{4}) brings us to a foliation whose sections are the Poincaré balls
$$ ds^2_{\rm{Mink}} = -d\tau^2 + \frac{8\rho}{(1-\rho^2)^2} d\tau d\rho + \textcolor{darkorange}{\frac{4}{(1-\rho^2)^2} \left( d\rho^2 + \rho^2 d\sigma^2\right)}.$$
Note that this metric is still the flat Minkowski metric with all its usual geometric properties, such as vanishing curvature. The important difference is that the points along the &lt;a href="https://en.wikipedia.org/wiki/End_%28topology%29" target="_blank" rel="noopener">ideal boundary&lt;/a> do not sit at spatial infinity but null infinity. The cuts $\rho=1$ along each $\tau$ slice corresponds to what is called a celestial sphere. This allows us to make the connection back to quantum gravity, because the celestial sphere is indeed the stage for an attempt at a &lt;a href="../../hyperboloidal-holography/">holographic description&lt;/a> called &lt;a href="https://arxiv.org/abs/2107.02075" target="_blank" rel="noopener">celestial holography&lt;/a>.&lt;/p>
&lt;p>The ideal boundary seems like an ideal place for quantum gravity.&lt;/p>
&lt;h3 id="hyperbolic-de-sitter">Hyperbolic de Sitter&lt;/h3>
&lt;p>The &lt;a href="https://www.quantamagazine.org/cosmologists-close-in-on-logical-laws-for-the-big-bang-20211110/" target="_blank" rel="noopener">Quanta article&lt;/a> discusses the difficulties in constructing an AdS/CFT correspondence in de Sitter spacetimes. Again, we can use coordinates to make the time slices of de Sitter hyperbolic, but the transformations are a bit tricky because de Sitter space has a different conformal geometry. For example, conformal infinity is spacelike (as opposed to timelike for AdS and null for Minkowski).&lt;/p>
&lt;p>There is another infinity in de Sitter that depends on coordinates and is useful for us. Consider the de Sitter metric in static coordinates
$$ ds^2_{\rm{dS}} = - \left(1-r^2\right) dt^2 + \frac{1}{1-r^2} dr^2 + r^2 d\sigma^2.$$
The singularity at $r=1$ corresponds to the &lt;a href="https://en.wikipedia.org/wiki/Cosmological_horizon" target="_blank" rel="noopener">cosmological horizon&lt;/a>. The horizon singularity is a coordinate singularity similar to the singularity at the black hole horizon of Schwarzschild spacetime.&lt;/p>
&lt;p>Here&amp;rsquo;s the transformation that gets us to hyperbolic time slices
$$ r = \sinh \xi \sinh \tau, \qquad \cosh t = \frac{\cosh \tau}{\sqrt{1-\sinh^2 \xi \sinh^2\tau}} $$
The transformed metric is comparatively simple
$$ ds^2_{\rm{dS}} = - d\tau^2 + \sinh ^2 \tau \left( d\xi^2 + \sinh^2\xi\ d\sigma^2 \right).$$
We recognize the hyperbolic geometry on $\tau=$const. slices. Alternatively, we can abuse notation and use the same letter for $r=\sinh\xi$ to get the purple representation of the hyperbolic metric
$$ ds^2_{\rm{dS}} = - d\tau^2 + \sinh ^2 \tau \left( \textcolor{purple}{\frac{1}{1+r^2} dr^2 + r^2 d\sigma^2} \right).$$
The Poincaré disk follows from a simple relabeling of the radial points (\ref{4}). This representation of the de Sitter metric is called &lt;a href="https://en.wikipedia.org/wiki/De_Sitter_space#Open_slicing" target="_blank" rel="noopener">open slicing&lt;/a>. I&amp;rsquo;m ignoring subtleties, such as the domain of transformations and the region covered by coordinates in the conformal diagram. We still see that we can choose to live in hyperbolic geometry if we want to, even in de Sitter spacetimes.&lt;/p>
&lt;p>The problem with the de Sitter metric in open slicing is its time-dependence. Especially for numerical studies, coordinates in which the metric is explicitly static are preferable. A better approach to construct a hyperbolic slicing is to consider the static patch and make a transformation similar to the flat case. First we push the horizon to infinity
$$ r_\ast = \int \frac{1}{1-r^2}dr = \mathrm{arctanh} \ r. $$
The de Sitter metric on the static patch becomes
$$ ds^2_{\mathrm{dS}} = \frac{1}{\cosh^2 r_\ast} \left( -dt^2 + dr_\ast^2 \right) + \tanh^2 r_\ast d\sigma^2. $$
Now we can do the same transformation as in (\ref{5}), take $\tau=$const., and get the following metric on the time slices
$$ \frac{1}{\cosh^2 r_\ast} \left(\frac{1}{1+r_\ast^2} dr_\ast^2 + \sinh^2 r_\ast d\sigma^2 \right). $$
This metric is not quite the hyperbolic metric, but has other nice properties for numerical calculations that I&amp;rsquo;ll write about in another post.&lt;/p>
&lt;h3 id="this-is-water">This is Water&lt;/h3>
&lt;p>The point of this post is that you can find suitable coordinates that make space hyperbolic. There is nothing to be jealous about Escher&amp;rsquo;s fish. It may well be that physically motivated time slices of flat spacetime are naturally hyperbolic but we don&amp;rsquo;t experience it because the speed of light is so large.&lt;/p>
&lt;p>Are these time transformations physically motivated? What do they mean? Doesn&amp;rsquo;t a preferred time slice contradict the principle of covariance? I&amp;rsquo;ll leave those questions for another post.&lt;/p>
&lt;p>Enjoy the swim.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>To get the disk model, just replace $d\sigma^2$ with $d\theta^2$.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>The fish could, in principle, find out that they do live in hyperbolic space by making local measurements of &lt;em>intrinsic&lt;/em> curvature.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Special relativity as hyperbolic geometry</title><link>https://anilzen.github.io/post/hyperbolic-relativity/</link><pubDate>Sat, 06 Nov 2021 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/post/hyperbolic-relativity/</guid><description>&lt;p>Space and time bend and stretch with every move.&lt;/p>
&lt;p>I was blown away by this as a young high schooler interested in physics and philosophy. To me, the recognition that natural laws dynamically govern space and time was a triumph of physics over philosophy, of Einstein over Kant, of empirical determination over &lt;a href="https://en.wikipedia.org/wiki/Analytic%E2%80%93synthetic_distinction#Kant%27s_version_and_the_a_priori_/_a_posteriori_distinction" target="_blank" rel="noopener">synthetic a priori&lt;/a>.&lt;/p>
&lt;p>As fascinated as I was by special relativity, the formalism seemed relatively obscure. I recognized patterns in the &lt;a href="https://en.wikipedia.org/wiki/Lorentz_transformation" target="_blank" rel="noopener">Lorentz transformations&lt;/a> but missed a deeper insight into the origins of this structure. At college, I considered various standard derivations of relativistic effects, such as the &lt;a href="https://en.wikipedia.org/wiki/Relativistic_aberration" target="_blank" rel="noopener">relativistic aberration&lt;/a> or the &lt;a href="https://en.wikipedia.org/wiki/Thomas_precession" target="_blank" rel="noopener">Thomas precession&lt;/a>, tedious and boring.&lt;/p>
&lt;p>A simple example is Einstein&amp;rsquo;s &lt;a href="https://en.wikipedia.org/wiki/Velocity-addition_formula" target="_blank" rel="noopener">velocity-addition formula&lt;/a>. In Galilean relativity, which corresponds to our everyday experience, we can just add velocities. In special relativity, the speed of light is constant, so the addition of velocities must be such that no composition of velocities exceeds the speed of light. In the simplest case of collinear motion (velocities pointing in the same direction), velocity addition becomes speed addition and looks like this
$$ \tag{1} \label{1}
u = \frac{u_1+u_2}{1+\tfrac{u_1 u_2}{c^2}} $$
The speed of light, $c$, can never be exceeded with this composition law. While we can understand the result, it is not intuitively clear how this addition rule is related to the structure of space and time.&lt;/p>
&lt;p>It turns out that relativistic calculations have natural interpretations in hyperbolic geometry. This viewpoint not only simplifies the calculations but also provides deeper insights into the theory. Its power extends beyond special relativity into machine learning and general relativity. And it all starts with Minkowski.&lt;/p>
&lt;h3 id="hyperbole">Hyperbole&lt;/h3>
&lt;p>In 1908, &lt;a href="https://en.wikipedia.org/wiki/Hermann_Minkowski" target="_blank" rel="noopener">Minkowski&lt;/a> gave a &lt;a href="https://www.math.nyu.edu/~tschinke/papers/yuri/14minkowski/raum-und-zeit.pdf" target="_blank" rel="noopener">lecture&lt;/a> about a new mathematical model for space and time. In his opening remarks, he stated boldly that, from now on, space and time are mere shadows of a more fundamental union: spacetime.&lt;/p>
&lt;blockquote>
&lt;p>Henceforth space by itself, and time by itself, are doomed to fade away into mere shadows, and only a kind of union of the two will preserve an independent reality.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Hermann_Minkowski" target="_blank" rel="noopener">Minkowski&lt;/a> (1908)&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/p>
&lt;/blockquote>
&lt;p>The quote is by now famous. It has made its way to various outlets since appearing on the cover page of Synge&amp;rsquo;s &lt;a href="https://www.amazon.com/Relativity-Special-J-L-Synge/dp/B000GP7PX4" target="_blank" rel="noopener">textbook on relativity&lt;/a> in 1956. But when Minkowski made the statement, it was considered hyperbole and encountered resistance from physicists and mathematicians alike.&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;p>The spacetime view became widely celebrated a few years after the lecture because of its central role in general relativity. In the same lecture, Minkowski also promoted a non-Euclidean approach to spacetime. As a Göttingen mathematics professor, he recognized the non-Euclidean geometry of special relativity. This idea, however, didn&amp;rsquo;t gain traction as widely as the idea of spacetime. Only in the last 20 years has there been renewed interest in the hyperbolic nature of special relativity.&lt;/p>
&lt;p>To see why hyperbolic geometry is the natural geometry for special relativity, consider a two-dimensional spacetime with coordinates $(t,x)$ and Minkowski metric
$$ ds^2 = -dt^2 + dx^2. $$
A natural object for a metric is the set of points at unit distance from the origin. In two-dimensional Euclidean space, we get a circle, $x^2+y^2=1$. In two-dimensional Minkowski space, we get a hyperbola&lt;br>
$$ \tag{2}\label{2} t^2 - x^2 = 1. $$
Minkowski&amp;rsquo;s drawing below shows this hyperbola for $t&amp;gt;0$. In higher dimensions, you get the one-sheeted &lt;a href="https://en.wikipedia.org/wiki/Hyperboloid" target="_blank" rel="noopener">hyperboloid&lt;/a>, also called hyperbolic hyperboloid.&lt;/p>
&lt;p>
&lt;figure id="figure-minkowskis-drawing-depicts-hyperbola-as-the-invariant-space-under-lorentz-transformations-from-minkowskis-modern-worldhttpshalshsarchives-ouvertesfrhalshs-01234434-by-scott-walter">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Minkowski Drawing" srcset="
/post/hyperbolic-relativity/MinkDrawing_hu140b8f32c633551bdc76727a7862e265_291670_b89ce1e1b459df492f891c8a09eaf42e.webp 400w,
/post/hyperbolic-relativity/MinkDrawing_hu140b8f32c633551bdc76727a7862e265_291670_4324b0371c578b6a70ed333dddf1eab4.webp 760w,
/post/hyperbolic-relativity/MinkDrawing_hu140b8f32c633551bdc76727a7862e265_291670_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/hyperbolic-relativity/MinkDrawing_hu140b8f32c633551bdc76727a7862e265_291670_b89ce1e1b459df492f891c8a09eaf42e.webp"
width="473"
height="500"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Minkowski&amp;rsquo;s drawing depicts hyperbola as the invariant space under Lorentz transformations. &lt;br>From &lt;a href="https://halshs.archives-ouvertes.fr/halshs-01234434/" target="_blank" rel="noopener">Minkowski&amp;rsquo;s modern world&lt;/a> by Scott Walter.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>In his lecture, Minkowski demonstrated that Lorentz transformations are hyperbolic rotations that leave the hyperbola (\ref{2}) invariant. He died in January 1909 before he could develop this idea further, but it was embraced by a few other scientists, particularly by &lt;a href="https://en.wikipedia.org/wiki/Arnold_Sommerfeld" target="_blank" rel="noopener">Sommerfeld&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/Vladimir_Vari%C4%87ak" target="_blank" rel="noopener">Varičak&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/Alfred_Robb" target="_blank" rel="noopener">Robb&lt;/a>, and &lt;a href="https://en.wikipedia.org/wiki/%C3%89mile_Borel" target="_blank" rel="noopener">Borel&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>The principle of relativity corresponds to the hypothesis that the kinematic space is a space of constant negative curvature, the space of Lobachevsky and Bolyai. The value of the radius of curvature is the speed of light.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/%C3%89mile_Borel" target="_blank" rel="noopener">Borel&lt;/a> (1913)&lt;/p>
&lt;/blockquote>
&lt;p>The mathematical formalism for a hyperbolic rotation is just like a regular rotation, except with hyperbolic functions instead of trigonometric functions. &lt;a href="https://en.wikipedia.org/wiki/Squeeze_mapping" target="_blank" rel="noopener">Hyperbolic functions&lt;/a> are related to their trigonometric counterparts through &lt;a href="https://en.wikipedia.org/wiki/Hyperbolic_functions#Complex_trigonometric_definitions" target="_blank" rel="noopener">complex angles&lt;/a>. An intuitive way to think about the hyperbolic rotation is the &lt;a href="https://en.wikipedia.org/wiki/Squeeze_mapping" target="_blank" rel="noopener">squeeze mapping&lt;/a>, which should be familiar to anyone who has seen a Lorentz transformation on a spacetime diagram.&lt;/p>
&lt;p>
&lt;figure id="figure-lorentz-boosts-are-squeeze-mappingshttpsenwikipediaorgwikisqueeze_mapping-from-wikimediahttpscommonswikimediaorgwikifileminkboost2gif">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Squeeze Mapping"
src="https://anilzen.github.io/post/hyperbolic-relativity/MinkBoost.gif"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Lorentz boosts are &lt;a href="https://en.wikipedia.org/wiki/Squeeze_mapping" target="_blank" rel="noopener">squeeze mappings&lt;/a>. &lt;br>From &lt;a href="https://commons.wikimedia.org/wiki/File:MinkBoost2.gif" target="_blank" rel="noopener">Wikimedia&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Consider the Lorentz boost with relative speed $v$ between frames
$$ L(v)= \begin{pmatrix}
\gamma &amp;amp; -\gamma \beta \\
-\gamma \beta &amp;amp; \gamma
\end{pmatrix}, $$
where we defined the rescaled speed $\beta:=\tfrac{v}{c}$ and the Lorentz factor $\gamma:=1/\sqrt{1-\beta^2}$. We can write it as a hyperbolic rotation with angle $w$:
$$ L(w)= \begin{pmatrix}
\cosh w &amp;amp; -\sinh w \\
-\sinh w &amp;amp; \cosh w
\end{pmatrix}, $$
where the Lorentz factor becomes $\gamma=\cosh w$ and the rescaled speed becomes $\beta=\tanh w$. So the Lorentz transformation is simply a rotation in hyperbolic space with an angle, $w$, related to the observer&amp;rsquo;s speed as $v=c \ \mathrm{arctanh}\ w$. Its inverse is just a rotation with the inverse angle, $L(w)^{-1}=L(-w)$. We can directly add the hyperbolic angles instead of dealing with weird addition formulas because composition of rotations along the same dimension is additive $L(w_1)L(w_2)=L(w_1+w_2)$.&lt;/p>
&lt;p>Physicists usually think in terms of velocities and speeds, not hyperbolic angles. Writing the angle addition in terms of speeds, we get
$$ \beta = \tanh (w_1+w_2) = \frac{\tanh w_1+\tanh w_2}{1+\tanh w_1 \tanh w_2} = \frac{\beta_1+\beta_2}{1+\beta_1 \beta_2}. $$
Einstein&amp;rsquo;s velocity addition (\ref{1}) becomes a consequence of hyperbolic identities.&lt;/p>
&lt;p>There is much more that one can simplify or understand with this viewpoint. For example, the formula for the Doppler shift is a simple $e^w$. The non-commutativity of general Lorentz boosts in higher dimensions becomes immediately apparent: Lorentz boosts do not commute just like rotations do not commute. The Thomas precession becomes easier to visualize as the consequence of parallel transport of vectors in the curved velocity space (&lt;a href="https://en.wikipedia.org/wiki/Holonomy" target="_blank" rel="noopener">holonomy transformation&lt;/a>). Many other calculations and ideas in special relativity translate to simple exercises in hyperbolic geometry.&lt;/p>
&lt;p>If you&amp;rsquo;re intrigued, follow the rabbit. Scott Walter&amp;rsquo;s papers are excellent for the historical developments: &lt;a href="https://halshs.archives-ouvertes.fr/halshs-00319209/" target="_blank" rel="noopener">Hermann Minkowski and the scandal of spacetime&lt;/a>, &lt;a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.535.6918&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="noopener">The Non-Euclidean Style of Minkowskian Relativity&lt;/a>, and &lt;a href="https://halshs.archives-ouvertes.fr/halshs-01234434/" target="_blank" rel="noopener">Minkowski&amp;rsquo;s modern world&lt;/a>. There are some gems in Barrett&amp;rsquo;s &lt;a href="https://arxiv.org/abs/1102.0462" target="_blank" rel="noopener">The Hyperbolic Theory of Special Relativity&lt;/a>. Rhodes and Semon go into those tedious calculations in &lt;a href="https://arxiv.org/abs/gr-qc/0501070" target="_blank" rel="noopener">Relativistic velocity space, Wigner rotation and Thomas precession&lt;/a>. Tevian Dray&amp;rsquo;s textbook on &lt;a href="https://www.amazon.com/Geometry-Special-Relativity-Tevian-Dray/dp/1466510471" target="_blank" rel="noopener">The Geometry of Special Relativity&lt;/a> from 2012 takes the hyperbolic viewpoint to teach special relativity.&lt;/p>
&lt;p>Ungar probably has the most influential and extensive work in this space. His books on &lt;a href="https://en.wikipedia.org/wiki/Gyrovector_space" target="_blank" rel="noopener">gyrovector space&lt;/a> present relativistic calculations in hyperbolic geometry from a solid group-theoretical viewpoint. Two of his relevant books are &lt;a href="https://www.worldscientific.com/worldscibooks/10.1142/6625" target="_blank" rel="noopener">Analytic Hyperbolic Geometry and Albert Einstein&amp;rsquo;s Special Theory of Relativity&lt;/a> from 2008 and &lt;a href="https://link.springer.com/book/10.1007/0-306-47134-5" target="_blank" rel="noopener">Beyond the Einstein Addition Law and its Gyroscopic Thomas Precession&lt;/a> from 2012. This work has even found applications in machine learning.&lt;/p>
&lt;h3 id="beyond-special">Beyond special&lt;/h3>
&lt;p>The hyperbolic view opens a new window to a universe beyond rewriting calculations in special relativity.&lt;/p>
&lt;p>Hyperbolic geometry efficiently represents &lt;a href="https://arxiv.org/abs/1006.5169" target="_blank" rel="noopener">hierarchical relationships and complex networks&lt;/a>. &lt;a href="https://arxiv.org/abs/1705.08039" target="_blank" rel="noopener">Nickel and Kiela&lt;/a> demonstrated in 2017 that hyperbolic geometry is better than Euclidean geometry when applying machine learning to complex data structures. In a &lt;a href="https://arxiv.org/abs/1806.03417" target="_blank" rel="noopener">follow-up paper&lt;/a>, they compared different hyperbolic models for learning hierarchical relationships and found that the Lorentz model is more efficient than the Poincaré model. Around the same time, &lt;a href="https://arxiv.org/abs/1805.09112" target="_blank" rel="noopener">Ganea, Bécigneul, and Hofmann&lt;/a> derived hyperbolic versions of deep learning tools by using Ungar&amp;rsquo;s gyrovector formalism.&lt;/p>
&lt;p>These papers opened up a new research direction in machine learning on &lt;a href="https://arxiv.org/abs/2101.04562" target="_blank" rel="noopener">hyperbolic deep neural networks&lt;/a>. The researchers use tools developed specifically for special relativity, such as Lorentz transformations and gyrovector spaces, to improve machine learning algorithms. Physicists usually associate velocities and universal speed limits with Lorentz transformations. It is fascinating that Lorentz transformations arise in problems without a notion of motion.&lt;/p>
&lt;p>Another application closer to my heart is hyperbolic geometry in general relativity. The spacetime of special relativity is special: we can identify Minkowski space with its tangent space which allows us to talk of spacetime events as four-vectors and apply global Lorentz transformations. These properties do not generalize to arbitrary Lorentzian manifolds. It&amp;rsquo;s not immediately evident that the hyperbolic viewpoint is still valuable when the spacetime is curved.&lt;/p>
&lt;p>As we have seen above, Minkowski&amp;rsquo;s hyperboloid (\ref{2}) consists of points at unit proper distance from the origin. Hyperboloids are as natural in Minkowski space as spheres are in Euclidean space. So &lt;strong>hyperboloidal&lt;/strong> coordinates may be as natural in Lorentzian manifolds as spherical coordinates are in Riemannian manifolds.&lt;/p>
&lt;p>How can we construct such hyperboloidal coordinates? In many applications, we need to separate spacetime into space and time. Let&amp;rsquo;s separate spacetime into hyperbolic space and time in non-Euclidean style. One approach is to take a hint from spherical coordinates. Generalize the hyperbola from (\ref{2}) to an arbitrary radius of curvature
$$ t^2-x^2 = \tau^2, $$
and use $\tau$ as a new coordinate. Setting $x=\tau \ \sinh \rho$ gives us the &lt;a href="https://en.wikipedia.org/wiki/Milne_model" target="_blank" rel="noopener">Milne universe&lt;/a> with metric
$$ ds^2 = -d\tau^2 + \tau^2 d\xi^2.$$
These coordinates have various use cases today, including &lt;a href="https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.21.392" target="_blank" rel="noopener">quantum field theory&lt;/a>, &lt;a href="https://arxiv.org/abs/2107.02075" target="_blank" rel="noopener">holography&lt;/a>, and the theory of &lt;a href="https://www.worldscientific.com/worldscibooks/10.1142/9427" target="_blank" rel="noopener">partial differential equations&lt;/a>.&lt;/p>
&lt;p>Another approach is to shift the hyperbola (\ref{2}) along the time direction by $\tau$ recognizing the special role of time
$$ (\tau - t)^2 - x^2 = 1 $$
Using $\tau$ as the new time coordinate, the Minkowski metric becomes
$$ ds^2 = -d\tau^2 + \frac{2x}{\sqrt{1+x^2}} d\tau dx + \frac{1}{1+x^2} dx^2.$$
Level sets of $\tau$ are hyperbolic spaces. We can bring the metric into the more recognizable Poincaré form by the spatial transformation $x=2 \rho/(1-\rho^2)$.
$$ ds^2 = -d\tau^2 + \frac{4}{(1-\rho^2)^2} \left(2\rho d\tau d\rho + d\rho^2\right).$$
This construction generalizes to &lt;a href="../../publication/zenginoglu-2008-hyperboloidal/">curved spacetimes&lt;/a>. It&amp;rsquo;s favorable for numerically solving wave equations because the coefficients don&amp;rsquo;t depend on time. Some of the problems appearing in standard coordinates, such as the outer boundary and radiation extraction problems, &lt;a href="../../publication/zenginoglu-2011-hyperboloidal/">don&amp;rsquo;t arise with this approach&lt;/a> because we can solve the wave equation on the infinite spatial domain $\rho\in[0,1]$.&lt;/p>
&lt;p>Hyperboloidal coordinates may seem unnatural at first, but that&amp;rsquo;s because we are used to thinking about space and time in Newtonian terms. Once you spend some time with them, you see that hyperboloidal coordinates are as valuable in Lorentzian manifolds as spherical coordinates are in Riemannian manifolds. I devoted most of my &lt;a href="../../publication/">work&lt;/a> exploring properties of these surfaces and I find their relation to special relativity fascinating.&lt;/p>
&lt;h3 id="hyperclusion">Hyperclusion&lt;/h3>
&lt;p>Physics doesn&amp;rsquo;t care about how we make calculations in special relativity. The Wikipedia page for the &lt;a href="https://en.wikipedia.org/wiki/History_of_Lorentz_transformations" target="_blank" rel="noopener">history of Lorentz transformations&lt;/a> lists many equivalent formalisms. We could keep the usual relativistic formulas using velocities and Lorentz factors, or employ &lt;a href="https://en.wikipedia.org/wiki/Bondi_k-calculus" target="_blank" rel="noopener">Bondi&amp;rsquo;s $k$-calculus&lt;/a>, or &lt;a href="https://en.wikipedia.org/wiki/Gyrovector_space" target="_blank" rel="noopener">Ungar&amp;rsquo;s gyrovector space&lt;/a>. Why choose one over the other?&lt;/p>
&lt;p>It may be just personal preference. How do you understand? I understand by analogy, so it&amp;rsquo;s helpful to me when formalisms lend themselves to connections to other fields.&lt;/p>
&lt;p>There is another, much more relevant reason to prefer the hyperbolic viewpoint. As scientists, we want powerful descriptions. The same thinking style should apply to a broad class of phenomena. In other words, we want to avoid overfitting our formalism to the phenomena at hand. Hyperbolic calculations translate into machine learning, spacetime curvature, and maybe other areas to be discovered. The non-Euclidean approach with hyperbolic angles seems more powerful than the usual approach with velocities.&lt;/p>
&lt;!-- [^3]: Scott Walter wrote a wonderful historical account of these developments in ["The Non-Euclidean Style of Minkowskian Relativity"](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.535.6918&amp;rep=rep1&amp;type=pdf).
[^4]: Walter, Scott. ["Minkowski’s modern world."](https://halshs.archives-ouvertes.fr/halshs-01234434/) Minkowski Spacetime: A Hundred Years Later. Springer, Dordrecht, 2010. 43-61. -->
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>The lecture was given on September 21, 1908 in Köln during a meeting of researchers of nature (the lovely German compound word for such a meeting is Naturforscherversammlung). The &lt;a href="https://www.math.nyu.edu/~tschinke/papers/yuri/14minkowski/raum-und-zeit.pdf" target="_blank" rel="noopener">German version&lt;/a> sounds stronger in its proclamation: &lt;em>&amp;ldquo;Von Stund&amp;rsquo; an sollen Raum für sich und Zeit für sich völlig zu Schatten herabsinken und nur noch eine Art Union der beiden soll Selbständigkeit bewahren.&amp;rdquo;&lt;/em>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Walter, Scott. &amp;ldquo;&lt;a href="https://halshs.archives-ouvertes.fr/halshs-00319209/" target="_blank" rel="noopener">Hermann Minkowski and the scandal of spacetime.&lt;/a>&amp;rdquo; ESI News 3.1 (2008): 6-8.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>A null infinity layer for wave scattering</title><link>https://anilzen.github.io/publication/zenginoglu-2021-null-infinity-layer/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/zenginoglu-2021-null-infinity-layer/</guid><description/></item><item><title>Hyperboloidal Holography</title><link>https://anilzen.github.io/post/hyperboloidal-holography/</link><pubDate>Thu, 07 Oct 2021 00:00:16 -0400</pubDate><guid>https://anilzen.github.io/post/hyperboloidal-holography/</guid><description>&lt;h3 id="brief-history-of-bekenstein-hawking-black-hole-entropy">Brief History of Bekenstein-Hawking Black-Hole Entropy&lt;/h3>
&lt;p>When physicists want to understand something, they consider idealizations and extremes. To understand spacetime then, consider black holes.&lt;/p>
&lt;blockquote>
&lt;p>The black holes of nature are the most perfect macroscopic objects there are in the universe:
the only elements in their construction are our concepts of space and time.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Subrahmanyan_Chandrasekhar" target="_blank" rel="noopener">Chandrasekhar&lt;/a> (1983)&lt;/p>
&lt;/blockquote>
&lt;p>Black holes are simple with just two properties: mass and rotation. By dropping an object into a black hole, you can increase its mass and change its rotation.&lt;/p>
&lt;p>The simplicity of black holes contradicts the second law of thermodynamics, which states that &lt;em>total entropy never decreases&lt;/em>. If black holes are that simple, you can reduce total entropy by dropping a high-entropy object, say a hot teacup, into the black hole.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/John_Archibald_Wheeler" target="_blank" rel="noopener">Wheeler&lt;/a>, best known for coining the term &amp;ldquo;black hole,&amp;rdquo; tasked one of his graduate students at Princeton, the young &lt;a href="https://en.wikipedia.org/wiki/Jacob_Bekenstein" target="_blank" rel="noopener">Bekenstein&lt;/a>, with resolving this paradox. The &lt;a href="https://en.wikipedia.org/wiki/History_of_general_relativity#Golden_age" target="_blank" rel="noopener">golden age&lt;/a> of relativity was in full swing around 1970, and Princeton was among the best places to be. &lt;a href="https://en.wikipedia.org/wiki/Demetrios_Christodoulou" target="_blank" rel="noopener">Christodoulou&lt;/a>, another student of Wheeler, and &lt;a href="https://en.wikipedia.org/wiki/Stephen_Hawking" target="_blank" rel="noopener">Hawking&lt;/a> had just shown that &lt;em>the area of a black hole horizon never decreases&lt;/em>. To resolve the entropy paradox, Bekenstein conjectured that black holes must have entropy and that the entropy must be proportional to the area of the black hole horizon in Planck units. The proportionality constant was determined by the discovery of &lt;a href="https://en.wikipedia.org/wiki/Hawking_radiation" target="_blank" rel="noopener">Hawking temperature&lt;/a>, which is why we call it the &lt;a href="http://www.scholarpedia.org/article/Bekenstein-Hawking_entropy" target="_blank" rel="noopener">Bekenstein-Hawking&lt;/a> Black-Hole entropy
$$ S_{BH} = \frac{A}{4 L_P^2}$$
where $A$ is the horizon area, $L_P$ is the &lt;a href="https://en.wikipedia.org/wiki/Planck_length" target="_blank" rel="noopener">Planck length&lt;/a>, $S$ stands for entropy (probably after &lt;a href="https://en.wikipedia.org/wiki/Nicolas_L%C3%A9onard_Sadi_Carnot" target="_blank" rel="noopener">Sadi Carnot&lt;/a>), and BH stands for &lt;a href="#brief-history-of-bekenstein-hawking-black-hole-entropy">Brief History&lt;/a>.&lt;/p>
&lt;p>You may think this is a curious result about curious objects called black holes with little relevance to our notion of spacetime. But consider the following question: what is the maximum entropy in a finite, say spherical region? If you take some high-entropy matter and squeeze it spherically (with your mind), eventually, you hit the Bekenstein-Hawking entropy, and the compressed matter forms a black hole. This thought experiment leads to the surprising conclusion that the entropy of any matter system is bound by the surface area of the smallest sphere that encloses it&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>We started with black holes and arrived at the&lt;/p>
&lt;h3 id="holographic-principle">Holographic Principle&lt;/h3>
&lt;p>Counterintuitively, the maximal entropy of a region scales with the area of its boundary, not its volume. The &lt;a href="https://en.wikipedia.org/wiki/Holographic_principle" target="_blank" rel="noopener">holographic principle&lt;/a> pushes this area-dependence further, suggesting that the dynamics in a volume of space is encoded on the boundary of that space. In other words, the information content of a spacetime region is encoded on the surface of that region.&lt;/p>
&lt;p>Take a break and think about this for a moment. How can our three-dimensional world be encoded on a two-dimensional surface? What does it mean? Is the universe a &lt;a href="https://en.wikipedia.org/wiki/Simulation_hypothesis" target="_blank" rel="noopener">game&lt;/a> running on the screen of an alien child? Are we trapped in &lt;a href="https://en.wikipedia.org/wiki/The_Matrix" target="_blank" rel="noopener">The Matrix&lt;/a>? Are we &lt;a href="https://www.youtube.com/watch?v=7nqcL0mjMjw" target="_blank" rel="noopener">Livin&amp;rsquo; on the Edge&lt;/a>?&lt;/p>
&lt;p>
&lt;figure id="figure-aerosmith-livin-on-the-edge-video-1993">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Living on the Edge" srcset="
/post/hyperboloidal-holography/living_on_the_edge_hu173cdc4707bba8c72fbdcae6744ed276_37858_c7a240253d106fdf4dedfdaa3a7ac972.webp 400w,
/post/hyperboloidal-holography/living_on_the_edge_hu173cdc4707bba8c72fbdcae6744ed276_37858_207a77ccf2abf58e87d6f804fcaac0b9.webp 760w,
/post/hyperboloidal-holography/living_on_the_edge_hu173cdc4707bba8c72fbdcae6744ed276_37858_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://anilzen.github.io/post/hyperboloidal-holography/living_on_the_edge_hu173cdc4707bba8c72fbdcae6744ed276_37858_c7a240253d106fdf4dedfdaa3a7ac972.webp"
width="722"
height="531"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Aerosmith: Livin&amp;rsquo; on the Edge (Video 1993)
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;h3 id="adscft-correspondence">AdS/CFT correspondence&lt;/h3>
&lt;p>The most famous realization of the holographic principle is the &lt;a href="https://en.wikipedia.org/wiki/AdS/CFT_correspondence" target="_blank" rel="noopener">AdS/CFT correspondence&lt;/a>. Conjectured by &lt;a href="https://en.wikipedia.org/wiki/Juan_Mart%C3%ADn_Maldacena" target="_blank" rel="noopener">Maldacena&lt;/a> in 1997, the correspondence suggests an equivalence between a &lt;a href="https://en.wikipedia.org/wiki/String_theory" target="_blank" rel="noopener">string theory&lt;/a> in curved anti-de Sitter spacetime (AdS) and a strongly coupled &lt;a href="https://en.wikipedia.org/wiki/Conformal_field_theory" target="_blank" rel="noopener">conformal field theory&lt;/a> in a lower-dimensional flat spacetime. Aside from the details, the remarkable aspect of the conjecture is the duality between the bulk and the boundary of AdS.&lt;/p>
&lt;p>Below is a visual image for the AdS$_3$/CFT$_2$ correspondence. The cylinder represents the three-dimensional AdS$_3$ spacetime. On the boundary of the cylinder lives the 1+1 dimensional CFT$_2$. Slices of constant time are &lt;a href="https://en.wikipedia.org/wiki/Hyperbolic_manifold" target="_blank" rel="noopener">hyperbolic manifolds&lt;/a> with negative curvature, drawn as &lt;a href="https://en.wikipedia.org/wiki/Poincar%C3%A9_disk_model" target="_blank" rel="noopener">Poincaré disks&lt;/a>. A gravity theory within the disks is dual to a gravity-free theory on the bounding circle. The boundary of space encodes the bulk.&lt;/p>
&lt;p>
&lt;figure id="figure-ads-is-hyperbolic-space-with-a-time-direction-wikimediahttpscommonswikimediaorgwikifileads3svg">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="AdS3" srcset="
/post/hyperboloidal-holography/AdS3_hu87bd1888389a6c14d9cc1117d0109a96_71192_61970cad7d4d717edc3a67b3ac88af73.webp 400w,
/post/hyperboloidal-holography/AdS3_hu87bd1888389a6c14d9cc1117d0109a96_71192_352cb1eb5025a0154dc7b8a7e0e7b971.webp 760w,
/post/hyperboloidal-holography/AdS3_hu87bd1888389a6c14d9cc1117d0109a96_71192_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/hyperboloidal-holography/AdS3_hu87bd1888389a6c14d9cc1117d0109a96_71192_61970cad7d4d717edc3a67b3ac88af73.webp"
width="472"
height="480"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
AdS is hyperbolic space with a time direction. &lt;a href="https://commons.wikimedia.org/wiki/File:AdS3.svg" target="_blank" rel="noopener">Wikimedia&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>The negative curvature of AdS is a feature and a bug. It allows the realization of the holographic principle due to its hyperbolic slices, but it also makes AdS an unphysical model for our universe. At cosmological scales, the universe has positive curvature and resembles de Sitter space. At astrophysical and smaller scales, isolated systems are modeled by asymptotically flat spacetimes. Either way, anti-de Sitter spacetime, with all its &lt;a href="https://arxiv.org/abs/1611.01118" target="_blank" rel="noopener">bizarre&lt;/a> features, is not a good model for physical reality.&lt;/p>
&lt;p>Can we establish the holographic principle for isolated systems?&lt;/p>
&lt;h3 id="hyperbolic-hyperboloidal">Hyperbolic Hyperboloidal&lt;/h3>
&lt;p>Consider the AdS$_3$ metric in static coordinates with curvature scale $L$
$$ ds^2_{\rm{AdS}} = - \left(L^2+r^2\right) d\tau^2 + \textcolor{RedViolet}{\frac{L^2}{L^2+r^2} dr^2 + r^2 d\theta^2},$$
Slices of constant time are hyperbolic spaces with negative curvature. Hyperbolic geometry has much more space near its conformal boundary than Euclidean space. It seems that flat spacetime with its Euclidean time slices and vanishing curvature would not allow such a description
$$ ds^2_{\rm{Mink}} = -dt^2 + dr^2 + r^2 d\theta^2 . $$&lt;/p>
&lt;p>But now consider the time-shifted spacetime hyperboloid with curvature radius $L$
$$ (\tau-t)^2 - r^2 = L^2 $$
This hypersurface is spacelike everywhere and extends to null infinity. Such surfaces are called hyperboloidal. Solving for $\tau$, we choose the negative square root, so the surfaces extend to future null infinity.
$$ \tau = t - \sqrt{L^2+r^2}.$$
In this hyperboloidal time, Minkowski metric has hyperbolic spatial slices
$$ ds^2_{\rm{Mink}} = -d\tau^2 - \frac{2 r}{\sqrt{L^2+r^2}} d\tau dr + \textcolor{RedViolet}{\frac{L^2}{L^2+r^2} dr^2 + r^2 d\theta^2}. $$
Just like AdS, flat spacetime takes the form of hyperbolic space with a time direction. You can draw the spacetime geometry as a cylinder, which is what Penrose did in his first publication on conformal infinity in 1963.&lt;/p>
&lt;p>
&lt;figure id="figure-the-cylinders-in-penroses-drawing-are-hyperbolic-spaces-with-a-time-direction-penrose-prl-1963httpsjournalsapsorgprlabstract101103physrevlett1066">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Penrose cylinder" srcset="
/post/hyperboloidal-holography/featured_hu7eb71e76e4f9e823419f039672d26a8d_30573_5c28c60ce0a2024da2163000044e7e0a.webp 400w,
/post/hyperboloidal-holography/featured_hu7eb71e76e4f9e823419f039672d26a8d_30573_dd8192eb1d14d6f9877e737a76389855.webp 760w,
/post/hyperboloidal-holography/featured_hu7eb71e76e4f9e823419f039672d26a8d_30573_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/hyperboloidal-holography/featured_hu7eb71e76e4f9e823419f039672d26a8d_30573_5c28c60ce0a2024da2163000044e7e0a.webp"
width="377"
height="365"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
The cylinders in Penrose&amp;rsquo;s drawing are hyperbolic spaces with a time direction. &lt;a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.10.66" target="_blank" rel="noopener">Penrose, PRL (1963)&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>The agreement of the geometry of hyperboloidal and AdS slices is suggestive. Calculations in the AdS/CFT literature performed on time slices may carry over to flat spacetime in hyperboloidal coordinates. One such example is the Ryu-Takayanagi conjecture.&lt;/p>
&lt;h3 id="rt-if-you-agree">RT if you agree&lt;/h3>
&lt;p>Ryu and Takayanagi (RT) have &lt;a href="https://arxiv.org/abs/hep-th/0603001" target="_blank" rel="noopener">proposed&lt;/a> a relation between geometry in AdS and entropy in CFT.&lt;/p>
&lt;p>One can interpret the Bekenstein-Hawking entropy as a measure of information lost to external observers. The black-hole horizon acts as a screen dividing space into two subsystems inside and outside the horizon. &lt;a href="https://en.wikipedia.org/wiki/Entropy_of_entanglement" target="_blank" rel="noopener">Entanglement entropy&lt;/a> is proportional to the area of this screen. The RT conjecture relates the entanglement entropy on CFT to the surface area of a screen in AdS that divides the AdS boundary into subdomains.&lt;/p>
&lt;p>Words don&amp;rsquo;t do it justice, so let&amp;rsquo;s look at a picture. Below is a diagram from the &lt;a href="https://arxiv.org/abs/hep-th/0603001" target="_blank" rel="noopener">RT paper&lt;/a> for AdS$_3$ /CFT$_2$.&lt;/p>
&lt;p>
&lt;figure id="figure-diagram-demonstrating-rt-proposal-on-ads_3cft_2-ryu-and-takayanagi-arxiv-2006httpsarxivorgabshep-th0603001">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="RT conjecture" srcset="
/post/hyperboloidal-holography/RT_diagram_huc187e2868a67d5cdf87ff9d98ee019fe_90520_3d67cdaa312f19f9d0c16a749eafda23.webp 400w,
/post/hyperboloidal-holography/RT_diagram_huc187e2868a67d5cdf87ff9d98ee019fe_90520_d2c76ec2eb6abc02b8c34de0f7348e36.webp 760w,
/post/hyperboloidal-holography/RT_diagram_huc187e2868a67d5cdf87ff9d98ee019fe_90520_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/hyperboloidal-holography/RT_diagram_huc187e2868a67d5cdf87ff9d98ee019fe_90520_3d67cdaa312f19f9d0c16a749eafda23.webp"
width="760"
height="487"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Diagram demonstrating RT proposal on AdS$_3$/CFT$_2$. &lt;a href="https://arxiv.org/abs/hep-th/0603001" target="_blank" rel="noopener">Ryu and Takayanagi, arXiv (2006)&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>The two subsystems $A$ and $B$ of CFT$_2$ live on the boundary of AdS$_3$. They are separated by a screen $\gamma_A$ required to be an extremal surface in the bulk AdS$_3$. An extremal &amp;ldquo;surface&amp;rdquo; on a slice of AdS$_3$ is a geodesic line connecting the meeting points of the domains $A$ and $B$ on the bounding circle. The RT proposal states that the entanglement entropy of subsystem $A$ is proportional to the area of the screen $\gamma_A$:
$$ S_A \sim \rm{Area\ of\ }\gamma_A $$
In this case, we are talking about the length of a geodesic on a time slice. You can easily see that the calculation does not change whether you are in AdS or hyperboloidal Minkowski. In the &lt;a href="https://en.wikipedia.org/wiki/Ryu%E2%80%93Takayanagi_conjecture#Example" target="_blank" rel="noopener">example on Wikipedia&lt;/a>, the AdS$_3$ metric is written in the spatial coordinate $\rho$ defined through $r=\sinh \rho$, so
$$ ds^2_{\rm{AdS}} = -\cosh^2\rho\ d\tau^2 + \textcolor{Blue}{d\rho^2 + \sinh^2\rho\ d\theta^2}. $$
The same spatial geometry is obtained in hyperboloidal coordinates in Minkowski&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>
$$ ds^2_{\rm{Mink}} = -d\tau^2 - \sinh\rho\ d\tau d\rho + \textcolor{Blue}{d\rho^2 + \sinh^2\rho\ d\theta^2}. $$
This flat metric describes a stack of hyperbolic disks, just like the AdS metric. The length of the geodesic and its relation to entanglement entropy are the same in hyperboloidal coordinates. The original RT proposal applies directly to flat spacetime!&lt;/p>
&lt;p>The RT proposal reveals an interesting interpretation for the area dependence of entropy. It is easier to understand that entanglement entropy should be proportional to the area of a screen and not to the volume behind it. But maybe we should ask first: What is volume?&lt;/p>
&lt;h3 id="thinking-outside-the-ball">Thinking outside the ball&lt;/h3>
&lt;p>When we think of volume, we typically envision a box or a ball and consider the space inside. Relativity modifies our intuition about space and time, so it is reasonable that our notion of volume is modified as well. The volume of a region $B$ is the integral of the volume form
$$ V_B = \int_{B} \sqrt{\det h} \ d\Sigma, $$
and depends on the spatial metric $h$. In flat space we have $\sqrt{\det h} =$ $r^2 \sin\theta$. The volume of a ball of radius $R$ gives us the usual formula that we memorize at school
$$ V_{B(R)} = \int_{0}^{R} \int_0^{\pi} \int_0^{2 \pi} r^2 \sin\theta \ dr d\theta d\varphi= 4\pi \int_{0}^{R} r^2 dr = \frac{4\pi}{3}R^3. $$
For the hyperboloidal foliation with $L=1$ we have
$$ h = \frac{1}{1+r^2}dr^2 + r^2 d\theta^2 + r^2 \sin^2\theta d\varphi^2,\quad \sqrt{\det h} = \frac{r^2 \sin\theta}{\sqrt{1+r^2}}. $$
The volume expression changes to
$$ V_{B(R)} = 4\pi \int_{0}^{R} \frac{r^2}{\sqrt{1+r^2}} dr. $$
We can get a sense for it using Taylor expansions. For small spheres with $R \ll 1$, we get the usual result
$$ V_{B(R)}=\frac{4\pi}{3}R^3+O(R^5).$$
For large spheres, however, the series expansion for $R\gg 1$ gives
$$ V_{B(R)}=2\pi R^2 + O(\ln R).$$
Remarkably, volume and area scale the same way near the asymptotic boundary in hyperboloidal coordinates.&lt;/p>
&lt;p>These are suggestive observations about the potential usefulness of hyperboloidal surfaces in holography. And indeed, there is a current effort using hyperboloidal surfaces for holography.&lt;/p>
&lt;h3 id="celestial-holography">Celestial Holography&lt;/h3>
&lt;p>In the context of quantum gravity, the holographic principle suggests duality between a gravity theory in the bulk and a quantum theory on the boundary. &lt;a href="https://arxiv.org/abs/2107.02075" target="_blank" rel="noopener">Celestial holography&lt;/a> suggests that asymptotically flat spacetimes are dual to a theory living on the &amp;ldquo;celestial sphere&amp;rdquo; at infinity.&lt;/p>
&lt;p>The theory uses Bondi coordinates along null infinity to discuss asymptotic symmetries, and switches to a hyperboloidal slicing to discuss massive particles. The slices they use have been first proposed as a cosmological model by &lt;a href="https://en.wikipedia.org/wiki/Milne_model" target="_blank" rel="noopener">Milne&lt;/a> in 1935. Milne slices are level sets of
$$ \tau^2 = t^2 - r^2.$$
Introducing the spatial coordinate
$$ \rho = \frac{r}{\sqrt{t^2-r^2}}, $$
the Minkowski metric takes the form
$$ ds^2_{\rm{Mink}} = -d\tau^2 + \tau^2 \left( \textcolor{RedViolet}{\frac{1}{1+\rho^2} d\rho^2 + \rho^2 d\sigma^2}\right). $$
Again, we recognize the representation of Minkowski spacetime as a stack of hyperbolic disks. But in this case, the metric coefficients depend on time $\tau$. And the mean extrinsic curvature of Milne slices vanishes asymptotically in time as $K=3/\tau$.&lt;/p>
&lt;p>I suspect that the hyperboloidal foliation we discussed &lt;a href="#hyperbolic-hyperboloidal">above&lt;/a> is better than Milne slicing for holography. Consider the Penrose diagram of Milne slices.
&lt;figure id="figure-penrose-diagram-of-milne-slicing-strominger-arxiv-2017httpsarxivorgabshep-th0603001">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Hyperbolic slices" srcset="
/post/hyperboloidal-holography/hyperbolic_hu07438a4e6e3cb093e518d73c9c90a40d_74988_db8958ccf0594bed0168ca2fed9d8f85.webp 400w,
/post/hyperboloidal-holography/hyperbolic_hu07438a4e6e3cb093e518d73c9c90a40d_74988_67332c32c03e6a61abc8f80af55f0e95.webp 760w,
/post/hyperboloidal-holography/hyperbolic_hu07438a4e6e3cb093e518d73c9c90a40d_74988_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/hyperboloidal-holography/hyperbolic_hu07438a4e6e3cb093e518d73c9c90a40d_74988_db8958ccf0594bed0168ca2fed9d8f85.webp"
width="444"
height="453"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Penrose diagram of Milne slicing. &lt;a href="https://arxiv.org/abs/hep-th/0603001" target="_blank" rel="noopener">Strominger, arXiv (2017)&lt;/a>
&lt;/figcaption>&lt;/figure>
The Penrose diagram and the metric reveal some undesirable features of Milne slices compared to the hyperboloidal foliation:&lt;/p>
&lt;ul>
&lt;li>The slices intersect at null infinity.&lt;/li>
&lt;li>The coordinates are time-dependent.&lt;/li>
&lt;li>There is no analog of the AdS curvature scale.&lt;/li>
&lt;/ul>
&lt;p>In contrast, the hyperboloidal foliation has constant mean curvature, $K=3/L$, playing the same role as the curvature scale in AdS geometry. The coordinates are independent of time and provide a smooth foliation of null infinity. Below are Penrose diagrams for such hyperboloidal, constant-mean-curvature foliations with different values of the mean extrinsic curvature, which acts as a dial between characteristic and Cauchy surfaces.&lt;/p>
&lt;p>
&lt;figure id="figure-penrose-diagrams-of-hyperboloidal-foliations-with-different-mean-extrinsic-curvatures-k632-zenginoğlu-arxiv-2008httpsarxivorgabs07124333">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Hyperboloidal foliation" srcset="
/post/hyperboloidal-holography/hyperboloidal_huddf2caadb7366214d7f4d1665843ed97_102448_3b2e6e1d322650da3e63826c855b461a.webp 400w,
/post/hyperboloidal-holography/hyperboloidal_huddf2caadb7366214d7f4d1665843ed97_102448_f6ff2e6e1abd3e3ba1c347060783a487.webp 760w,
/post/hyperboloidal-holography/hyperboloidal_huddf2caadb7366214d7f4d1665843ed97_102448_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/hyperboloidal-holography/hyperboloidal_huddf2caadb7366214d7f4d1665843ed97_102448_3b2e6e1d322650da3e63826c855b461a.webp"
width="760"
height="338"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Penrose diagrams of hyperboloidal foliations with different mean extrinsic curvatures $K={6,3,2}$. &lt;a href="https://arxiv.org/abs/0712.4333" target="_blank" rel="noopener">Zenginoğlu, arXiv (2008)&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Milne coordinates have one advantage that a hyperboloidal foliation of null infinity cannot provide. A massive particle in constant motion will asymptote to a point on the Milne slices. Dirac used this property in his 1949 paper on &lt;a href="https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.21.392" target="_blank" rel="noopener">Forms of Relativistic Dynamics&lt;/a> and it appears repeatedly in celestial holography. I&amp;rsquo;m not sure how important this is. But Milne coordinates are also hyperboloidal. So in that sense, celestial holography is already using hyperboloidal holography.&lt;/p>
&lt;p>Celestial holography is relatively new and developing rapidly. We don&amp;rsquo;t know how the final version will look like. That&amp;rsquo;s the excitement of research.&lt;/p>
&lt;h3 id="wrap-up">Wrap up&lt;/h3>
&lt;p>A few curious observations suggest that hyperboloidal coordinates are interesting for holography.&lt;/p>
&lt;ul>
&lt;li>Flat spacetime becomes a stack of hyperbolic disks, with a &lt;a href="#hyperbolic-hyperboloidal">cylinder&lt;/a> representing the global picture.&lt;/li>
&lt;li>The original &lt;a href="#rt-if-you-agree">Ryu-Takayanagi calculations&lt;/a> apply to Minkowski spacetime.&lt;/li>
&lt;li>&lt;a href="#thinking-outside-the-ball">Volume scales as area&lt;/a> near the conformal boundary.&lt;/li>
&lt;li>Spacetime hyperboloids are essential in &lt;a href="#celestial-holography">celestial holography&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>Are these observations indicative of a profound duality, an underlying realization of the holographic principle for isolated systems in hyperboloidal coordinates? I don&amp;rsquo;t know. The similarity between Minkowski in hyperboloidal coordinates and AdS is interesting, but it can also be misleading.&lt;/p>
&lt;p>The observations here are, by their very nature, coordinate-dependent. Suitable coordinates may ease calculations and suggest research directions, but the results should not depend on them. A significant, coordinate-independent difference to the AdS case is the null conformal boundary of asymptotically flat spacetimes. Any realization of the holographic principle will need to incorporate the null boundary and the associated &lt;a href="https://en.wikipedia.org/wiki/Bondi%E2%80%93Metzner%E2%80%93Sachs_group" target="_blank" rel="noopener">symmetries&lt;/a> as celestial holography is attempting to do.&lt;/p>
&lt;p>It can be misleading to take similarities in specific coordinates too far, but it can also be a missed opportunity to ignore them. I think hyperbolic geometry and hyperboloidal surfaces will keep playing an essential role in holography. &lt;a href="https://www.urbandictionary.com/define.php?term=Watch%20this%20Space" target="_blank" rel="noopener">Watch this space!&lt;/a>&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>There are various subtleties here that I&amp;rsquo;m glossing over. You can go down the rabbit hole following the &lt;a href="http://www.scholarpedia.org/article/Bekenstein_bound" target="_blank" rel="noopener">Bekenstein bound&lt;/a>.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Setting the curvature radius to unity for simplicity, $L=1$.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>How far is infinity?</title><link>https://anilzen.github.io/post/empirical-infinity/</link><pubDate>Wed, 08 Sep 2021 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/post/empirical-infinity/</guid><description>&lt;p>Most of my research uses infinity in numerical computations. When presenting my work, I often encounter the objection that infinity is not physical. But not all infinities are created equal, and some are more physical than others. In fact, we can &lt;strong>verify by experiment&lt;/strong> that we are at infinity with respect to distant sources of radiation.&lt;/p>
&lt;p>Below I collect some thoughts on infinity and describe the experiment. Feel free to skip my ramblings and go directly to the &lt;a href="#a-thought-experiment">experiment&lt;/a>.&lt;/p>
&lt;h3 id="we-dont-need-no-infinity">We don&amp;rsquo;t need no infinity&lt;/h3>
&lt;p>Physicists are generally uncomfortable with infinity. First, it is argued, infinity cannot be the outcome of a physical experiment. Second, infinity as the outcome of a calculation indicates problems with the theory. Resolving such problems is important because they may reveal new research directions such as &lt;a href="https://en.wikipedia.org/wiki/Renormalization" target="_blank" rel="noopener">renormalization techniques&lt;/a>. Since infinity doesn&amp;rsquo;t make sense in experiment and should be avoided in theory, physicists are skeptical when it appears in mathematical descriptions of physical phenomena. A few examples: Max Tegmark argues that infinity is an idea that should be &lt;a href="https://www.edge.org/response-detail/25344" target="_blank" rel="noopener">retired from physics&lt;/a>; Nicolas Gisin aims to &lt;a href="https://www.quantamagazine.org/does-time-really-flow-new-clues-come-from-a-century-old-approach-to-math-20200407/" target="_blank" rel="noopener">understand time&lt;/a> without relying on infinitely precise numbers; and here is a quote from &lt;a href="https://www.nature.com/articles/s41567-018-0238-1" target="_blank" rel="noopener">The physics of infinity&lt;/a> by Ellis, Meissner, and Nicolai:&lt;/p>
&lt;blockquote>
&lt;p>We maintain that in each physical case where $\infty$ is used in a discussion, greater insight is attained by considering what large number $N$ will suffice instead because real physics is embodied in that number.&lt;/p>
&lt;/blockquote>
&lt;p>This uneasiness of physicists with infinity is also present in mathematics. There are mathematical philosophies and formalisms without infinity, such as &lt;a href="https://en.wikipedia.org/wiki/Finitism" target="_blank" rel="noopener">finitism&lt;/a> or &lt;a href="https://en.wikipedia.org/wiki/Intuitionism" target="_blank" rel="noopener">intuitionism&lt;/a>. I was attracted to similar ideas when I was in college, so I went down the rabbit hole of &lt;a href="https://en.wikipedia.org/wiki/Fuzzy_number" target="_blank" rel="noopener">fuzzy numbers&lt;/a>. They seemed to &amp;ldquo;naturally&amp;rdquo; represent physical measurements, which are not real numbers with infinite precision but fuzzy numbers with uncertainties. The more I learned about fuzzy numbers, however, the less I was convinced of their general utility. They were cumbersome and did not improve calculations in most cases. I learned, by way of fuzzy numbers, that successful mathematical representations do not necessarily match our physical intuitions. The map is not the territory.&lt;/p>
&lt;h3 id="empirical-infinity">Empirical infinity&lt;/h3>
&lt;p>Aristotle distinguishes &lt;a href="https://en.wikipedia.org/wiki/Actual_infinity" target="_blank" rel="noopener">two notions of infinity&lt;/a>: actual and potential. Potential infinity is what children learn first: numbers just keep going &amp;ldquo;forever&amp;rdquo;. Actual infinity is the completion of this process: the set of all numbers as a mathematical object.&lt;/p>
&lt;p>We do not observe infinity directly but we use it to represent our impressions of the natural world. In a painting, both sides of a road may meet at a &lt;a href="https://en.wikipedia.org/wiki/Vanishing_point" target="_blank" rel="noopener">vanishing point&lt;/a>. The road ends somewhere but the painting suggests that the meeting point is beyond the horizon of perception. The representation of &amp;ldquo;far&amp;rdquo; on the canvas is infinity.&lt;/p>
&lt;p>The infinity on the canvas is neither potential (it&amp;rsquo;s a finite, completed representation of infinity) nor actual (the horizon is actually at a finite distance). Call it empirical infinity. Empirical concepts are abstracted from individual perceptions. Empirical infinity is the best canvas representation of your perception looking down a long, seemingly unending road. You don&amp;rsquo;t know how far you can see, and you don&amp;rsquo;t care about representing that particular detail in your painting. You just want to express that it&amp;rsquo;s very far.&lt;/p>
&lt;h3 id="a-thought-experiment">A thought experiment&lt;/h3>
&lt;p>Black hole perturbations have a property that is different at infinity and at a finite distance. We can use this property to determine where we are with respect to the black hole. We would need very accurate detectors to measure it, which we don&amp;rsquo;t have yet, so consider this a reasonable thought experiment.&lt;/p>
&lt;p>Perturbations of a black hole go through an oscillatory period, and subsequently decay with a power-law $ \sim t^p$, where $t$ is time as measured by the observer and $p$ is the negative power&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Surprisingly, the power $p$ is different depending on where you observe the perturbations: -3 at finite distances, -2 at infinity&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. The slower decay at infinity is somewhat unexpected. One could argue that the power-law decay, which was first discovered for finite distances, should be non-radiative; it shouldn&amp;rsquo;t even be detected at infinity. As demonstrated by Gundlach, Price, and Pullin in 1994, it is not only radiative but decays slower at infinity than at finite distances&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. There are explanations of this slower decay in terms of accumulation of backscattering off of curvature towards infinity, but this is not our main concern&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Given that the rates at finite distances and infinity are different, we ask the following question:&lt;/p>
&lt;p>&lt;em>Which decay rate would we observe?&lt;/em>&lt;/p>
&lt;p>We are clearly a finite distance away from the black hole, so the intuitive answer is that we would measure the finite distance rate. However, the correct answer is the infinity rate.&lt;/p>
&lt;p>Below is a figure from a &lt;a href="https://anilzen.github.io/publication/zenginoglu-2008-tail/">paper of mine&lt;/a> where I studied various decay rates. The figure shows the evolution of the decay rate in time for various distances to the black hole.&lt;/p>
&lt;p>
&lt;figure id="figure-decay-rates-in-time-at-various-distances-from-the-source">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Decay rates by distance" srcset="
/post/empirical-infinity/featured_hucb430a8324f06a774b153a0b9634005a_69007_bf581f31ae27b9bf532bd87e03782aca.webp 400w,
/post/empirical-infinity/featured_hucb430a8324f06a774b153a0b9634005a_69007_12160c7bebd609602e4ccf6e0e0f676c.webp 760w,
/post/empirical-infinity/featured_hucb430a8324f06a774b153a0b9634005a_69007_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/empirical-infinity/featured_hucb430a8324f06a774b153a0b9634005a_69007_bf581f31ae27b9bf532bd87e03782aca.webp"
width="711"
height="440"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Decay rates in time at various distances from the source.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>The long-time decay rate is -3 for any finite distance as the theory demonstrates. But the theory is valid for the asymptotic limit. Initially, the decay rate for distant observers is much closer to -2 than to -3. The top curve in the figure is the measurement at infinity, and the next closest curve is at 40,000 units of black hole mass (in these units, a solar mass corresponds to a distance of 1.5 km). For a small black hole of 5 solar masses, this observer is about 300,000 km away. This may sound far, but it&amp;rsquo;s just one light second, which is about the distance between the Earth and the Moon. To compare, typical sources of gravitational waves are thousands and millions of light years away. The decay rate of perturbations from such a distant source would be indistinguishable from the infinity value.&lt;/p>
&lt;p>Infinity in astrophysics may be closer than the Moon.&lt;/p>
&lt;h3 id="wrap-up">Wrap-up&lt;/h3>
&lt;p>If we ever get to measure the power-law decay from black-hole perturbations, we will measure the infinity value and not the finite distance value, even though we are a finite distance away from the sources. Given that the distances are literally astronomical, it&amp;rsquo;s more practical to use infinity both in our calculations and in our discussions where it&amp;rsquo;s needed, instead of artificially avoiding it.&lt;/p>
&lt;p>I think most people agree that actual infinity doesn&amp;rsquo;t exist in the real world. But the number 3 also doesn&amp;rsquo;t exist in the real world. Numbers are useful concepts abstracted from observations and represented in mathematical models. The decay rate experiment demonstrates that infinity is a useful concept in describing astrophysical observations. We should embrace empirical infinity and its appearance in our formalisms as long as it improves our calculations.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>This was first discussed in detail by Richard Price in his 1972 paper &lt;a href="https://journals.aps.org/prd/abstract/10.1103/PhysRevD.5.2419" target="_blank" rel="noopener">Nonspherical Perturbations of Relativistic Gravitational Collapse. I. Scalar and Gravitational Perturbations&lt;/a>. The decay rate depends on initial data and mode of perturbation.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>The observation of a radiative field at infinity gives the part of the field that falls off as $1/r$. In other words, for a radiative field $\phi$, its observation at infinity is $\lim_{r\to\infty} r \phi$.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>C. Gundlach, R. H. Price, and J. Pullin. &lt;a href="https://journals.aps.org/prd/abstract/10.1103/PhysRevD.49.883" target="_blank" rel="noopener">Late-time behavior of stellar collapse and explosions. I. Linearized perturbations.&lt;/a> Physical Review D 49.2, 883 (1994).&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>For those of you who are wondering about the mathematical form of such a function, here is an example:
$$ \phi(t,r) \sim \frac{C_1}{(C_2+t+r)(C_3+t-r)^2} $$
The constants $C_i$ are arbitrary values determined by the initial perturbation. For fixed $r$ we have $\phi\sim 1/t^3$. Along null directions with constant $t+r$, the function decays as $\phi\sim 1/(t-r)^2$. Therefore, an observer measures the decay rate $3$ or $2$ depending on location.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Cauchy-horizon singularity inside perturbed Kerr black holes</title><link>https://anilzen.github.io/publication/burko-2016-cauchy/</link><pubDate>Tue, 09 Feb 2016 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/burko-2016-cauchy/</guid><description/></item><item><title>Testing numerically the null Cauchy horizon singularity inside Kerr black holes</title><link>https://anilzen.github.io/publication/burko-2015-testing/</link><pubDate>Wed, 01 Apr 2015 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/burko-2015-testing/</guid><description/></item><item><title>The antikick strikes back: Recoil velocities for nearly extremal binary black hole mergers in the test-mass limit</title><link>https://anilzen.github.io/publication/nagar-2014-antikick/</link><pubDate>Wed, 24 Dec 2014 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/nagar-2014-antikick/</guid><description/></item><item><title>Nondispersive decay for the cubic wave equation</title><link>https://anilzen.github.io/publication/donninger-2014-nondispersive/</link><pubDate>Fri, 30 May 2014 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/donninger-2014-nondispersive/</guid><description/></item><item><title>A new gravitational wave generation algorithm for particle perturbations of the Kerr spacetime</title><link>https://anilzen.github.io/publication/harms-2014-new/</link><pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/harms-2014-new/</guid><description/></item><item><title>Effective-one-body model for black-hole binaries with generic mass ratios and spins</title><link>https://anilzen.github.io/publication/pan-2014-eob/</link><pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/pan-2014-eob/</guid><description/></item><item><title>Intermediate behavior of Kerr tails</title><link>https://anilzen.github.io/publication/zenginoglu-2014-intermediate/</link><pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/zenginoglu-2014-intermediate/</guid><description/></item><item><title>Self-force via Green functions and worldline integration</title><link>https://anilzen.github.io/publication/wardell-2014-self/</link><pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/wardell-2014-self/</guid><description/></item><item><title>Stability of nonspinning effective-one-body model in approximating two-body dynamics and gravitational-wave emission</title><link>https://anilzen.github.io/publication/pan-2014-stability/</link><pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/pan-2014-stability/</guid><description/></item><item><title>Template banks for binary black hole searches with numerical relativity waveforms</title><link>https://anilzen.github.io/publication/kumar-2014-template/</link><pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/kumar-2014-template/</guid><description/></item><item><title>Catalog of 174 binary black hole simulations for gravitational wave astronomy</title><link>https://anilzen.github.io/publication/mroue-2013-catalog/</link><pubDate>Wed, 11 Dec 2013 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/mroue-2013-catalog/</guid><description/></item><item><title>Caustic echoes and their astrophysical applications</title><link>https://anilzen.github.io/publication/galley-2013-caustic/</link><pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/galley-2013-caustic/</guid><description/></item><item><title>Caustic Echoes as X-ray follow-ups from Sagittarius A</title><link>https://anilzen.github.io/publication/galley-2013-caustic-aas/</link><pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/galley-2013-caustic-aas/</guid><description/></item><item><title>Error-analysis and comparison to analytical models of numerical waveforms produced by the NRAR Collaboration</title><link>https://anilzen.github.io/publication/hinder-2013-error/</link><pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/hinder-2013-error/</guid><description/></item><item><title>Intermediate behavior of Kerr Black Hole tails</title><link>https://anilzen.github.io/publication/zenginoglu-2013-intermediate/</link><pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/zenginoglu-2013-intermediate/</guid><description/></item><item><title>Quasinormal modes of nearly extremal Kerr spacetimes: spectrum bifurcation and power-law ringdown</title><link>https://anilzen.github.io/publication/yang-2013-quasinormal/</link><pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/yang-2013-quasinormal/</guid><description/></item><item><title>Caustic echoes from a Schwarzschild black hole</title><link>https://anilzen.github.io/publication/zenginoglu-2012-caustic/</link><pubDate>Tue, 18 Sep 2012 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/zenginoglu-2012-caustic/</guid><description/></item><item><title>Horizon-absorption effects in coalescing black-hole binaries: An effective-one-body study of the nonspinning case</title><link>https://anilzen.github.io/publication/bernuzzi-2012-horizon/</link><pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/bernuzzi-2012-horizon/</guid><description/></item><item><title>Null infinity waveforms from extreme-mass-ratio inspirals in Kerr spacetime</title><link>https://anilzen.github.io/publication/zenginoglu-2011-null/</link><pubDate>Mon, 12 Sep 2011 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/zenginoglu-2011-null/</guid><description/></item><item><title>A geometric framework for black hole perturbations</title><link>https://anilzen.github.io/publication/zenginoglu-2011-geometric/</link><pubDate>Fri, 17 Jun 2011 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/zenginoglu-2011-geometric/</guid><description/></item><item><title>Hyperboloidal layers for hyperbolic equations on unbounded domains</title><link>https://anilzen.github.io/publication/zenginoglu-2011-hyperboloidal/</link><pubDate>Sun, 20 Mar 2011 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/zenginoglu-2011-hyperboloidal/</guid><description/></item><item><title>A physically motivated framework to describe black hole perturbations</title><link>https://anilzen.github.io/publication/zenginoglu-2011-physically/</link><pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/zenginoglu-2011-physically/</guid><description/></item><item><title>Binary black hole coalescence in the extreme-mass-ratio limit: testing and improving the effective-one-body multipolar waveform</title><link>https://anilzen.github.io/publication/bernuzzi-2011-binary/</link><pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/bernuzzi-2011-binary/</guid><description/></item><item><title>Binary black hole coalescence in the large-mass-ratio limit: the hyperboloidal layer method and waveforms at null infinity</title><link>https://anilzen.github.io/publication/bernuzzi-2011-layer/</link><pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/bernuzzi-2011-layer/</guid><description/></item><item><title>Saddle-point dynamics of a Yang—Mills field on the exterior Schwarzschild spacetime</title><link>https://anilzen.github.io/publication/bizon-2010-saddle/</link><pubDate>Fri, 16 Jul 2010 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/bizon-2010-saddle/</guid><description/></item><item><title>Asymptotics of Schwarzschild black hole perturbations</title><link>https://anilzen.github.io/publication/zenginoglu-2010-asymptotics/</link><pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/zenginoglu-2010-asymptotics/</guid><description/></item><item><title>Hyperboloidal evolution of test fields in three spatial dimensions</title><link>https://anilzen.github.io/publication/zenginoglu-2010-hyperboloidal/</link><pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/zenginoglu-2010-hyperboloidal/</guid><description/></item><item><title>Universality of global dynamics for the cubic wave equation</title><link>https://anilzen.github.io/publication/bizon-2009-universality/</link><pubDate>Thu, 10 Sep 2009 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/bizon-2009-universality/</guid><description/></item><item><title>Spacelike matching to null infinity</title><link>https://anilzen.github.io/publication/zenginoglu-2009-spacelike/</link><pubDate>Wed, 29 Jul 2009 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/zenginoglu-2009-spacelike/</guid><description/></item><item><title>Gravitational perturbations of Schwarzschild spacetime at null infinity and the hyperboloidal initial value problem</title><link>https://anilzen.github.io/publication/zenginoglu-2009-gravitational/</link><pubDate>Tue, 13 Jan 2009 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/zenginoglu-2009-gravitational/</guid><description/></item><item><title>A hyperboloidal study of tail decay rates for scalar and Yang–Mills fields</title><link>https://anilzen.github.io/publication/zenginoglu-2008-tail/</link><pubDate>Tue, 19 Aug 2008 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/zenginoglu-2008-tail/</guid><description/></item><item><title>Hyperboloidal foliations and scri-fixing</title><link>https://anilzen.github.io/publication/zenginoglu-2008-hyperboloidal/</link><pubDate>Thu, 19 Jun 2008 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/zenginoglu-2008-hyperboloidal/</guid><description/></item><item><title>Hyperboloidal evolution with the Einstein equations</title><link>https://anilzen.github.io/publication/zenginoglu-2008-einstein/</link><pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/zenginoglu-2008-einstein/</guid><description/></item><item><title>A conformal approach to numerical calculations of asymptotically flat spacetimes</title><link>https://anilzen.github.io/publication/zenginoglu-2007-conformal/</link><pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/zenginoglu-2007-conformal/</guid><description/></item><item><title>Numerical calculations near spatial infinity</title><link>https://anilzen.github.io/publication/zenginoglu-2007-numerical/</link><pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/zenginoglu-2007-numerical/</guid><description/></item><item><title>Hyperboloidal data and evolution</title><link>https://anilzen.github.io/publication/husa-2006-hyperboloidal/</link><pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/husa-2006-hyperboloidal/</guid><description/></item><item><title>Ideal magnetohydrodynamics in curved spacetime</title><link>https://anilzen.github.io/publication/zenginoglu-2003-ideal/</link><pubDate>Wed, 01 Jan 2003 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/publication/zenginoglu-2003-ideal/</guid><description/></item></channel></rss>